<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[懒程序员改变世界]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://qinghua.github.io/"/>
  <updated>2016-02-26T13:18:22.000Z</updated>
  <id>http://qinghua.github.io/</id>
  
  <author>
    <name><![CDATA[Qinghua Gao]]></name>
    <email><![CDATA[ggggqh666@163.com]]></email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[看例子学sed]]></title>
    <link href="http://qinghua.github.io/sed/"/>
    <id>http://qinghua.github.io/sed/</id>
    <published>2016-02-26T13:18:07.000Z</published>
    <updated>2016-02-26T13:18:22.000Z</updated>
    <content type="html"><![CDATA[<p>这次重温一下Linux下一个很老（反正比我老）很有用的流编辑器：sed（stream editor）。要是不经常使用，很容易忘记。可以把本文当成一个例子库，有用的时候来查一下。<br><a id="more"></a></p>
<p>假设我们有一个csv文件如下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;staff.csv</span><br><span class="line">Gavo,<span class="number">35</span></span><br><span class="line">Jane,<span class="number">21</span></span><br><span class="line">Bill,<span class="number">25</span></span><br><span class="line">Jimmy,<span class="number">42</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<h2 id="u589E"><a href="#u589E" class="headerlink" title="增"></a>增</h2><h3 id="u5728_u6587_u4EF6_u5934/_u5C3E_u589E_u52A0_u4E00_u884C"><a href="#u5728_u6587_u4EF6_u5934/_u5C3E_u589E_u52A0_u4E00_u884C" class="headerlink" title="在文件头/尾增加一行"></a>在文件头/尾增加一行</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'1i Name,Age'</span> staff.csv</span><br><span class="line">cat staff.csv</span><br></pre></td></tr></table></figure>
<p>如果没有<code>-i</code>，第一行命令会输出新的文件内容但不会改变<code>staff.csv</code>。<code>1i</code>中的<code>1</code>是指第1行，<code>i</code>是指在读取文件此行前增加（include）记录。如果把<code>i</code>换成<code>a</code>，指的是读取文件此行后增加（append）记录。是不是有点vi的感觉？下面这条命令的结果就会把新行插入到第三行：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'2a Hetty,29'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>要是想在文件尾增加一行的话，用：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'$a Hetty,29'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>其中的<code>$</code>指最后一行。如果要在倒数第二行增加一行呢？把<code>a</code>换成<code>i</code>吧。倒数第三行呢？你确认你真的有这么奇葩的需求么…</p>
<h3 id="u5728_u5339_u914D_u7684_u5730_u65B9_u589E_u52A0_u4E00_u884C"><a href="#u5728_u5339_u914D_u7684_u5730_u65B9_u589E_u52A0_u4E00_u884C" class="headerlink" title="在匹配的地方增加一行"></a>在匹配的地方增加一行</h3><p>如果我们要在Jane上面增加一行，这么做：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Jane/i Hetty,29'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>意思是当匹配到<code>Jane</code>的时候，便做后面的操作。接下来的<code>i</code>不用说了吧，也能替换成<code>a</code>。这里的匹配指的是部分匹配，也能匹配多行。试试下列命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/21/i Hetty,29'</span> staff.csv    <span class="comment"># 部分匹配</span></span><br><span class="line">sed <span class="string">'/J/i Hetty,29'</span> staff.csv     <span class="comment"># 匹配了两行</span></span><br></pre></td></tr></table></figure></p>
<p>下面这个命令可以在Gavo后面增加两行：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Gavo/a Hetty,29\nEmma,45'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<h2 id="u5220"><a href="#u5220" class="headerlink" title="删"></a>删</h2><h3 id="u5220_u9664_u7279_u5B9A_u884C"><a href="#u5220_u9664_u7279_u5B9A_u884C" class="headerlink" title="删除特定行"></a>删除特定行</h3><p>删（delete）和<a href="/sed/#u589E">増</a>很相似，区别是把<code>i</code>或<code>a</code>换成<code>d</code>即可：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'1d'</span> staff.csv         <span class="comment"># 删除第一行</span></span><br><span class="line">sed <span class="string">'$d'</span> staff.csv         <span class="comment"># 删除最后一行</span></span><br><span class="line">sed <span class="string">'/Jane/d'</span> staff.csv    <span class="comment"># 删除包含Jane的一行</span></span><br><span class="line">sed <span class="string">'/21/d'</span> staff.csv      <span class="comment"># 删除包含21的一行</span></span><br><span class="line">sed <span class="string">'/J/d'</span> staff.csv       <span class="comment"># 删除包含J的两行</span></span><br></pre></td></tr></table></figure></p>
<h3 id="u5220_u9664_u5173_u8054_u884C"><a href="#u5220_u9664_u5173_u8054_u884C" class="headerlink" title="删除关联行"></a>删除关联行</h3><p>下面这个命令把Gavo这一行和下一行都删掉：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Gavo/&#123;N;d;&#125;'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>其中的<code>N</code>就是下一行（next line）的意思。如果不想删除Gavo这行，用这个命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Gavo/&#123;N;s/\n.*//;&#125;'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>相当于匹配了两行也就是<code>Gavo,35\nJane,21</code>之后，再把<code>\n</code>之后的所有文本替换成空白，即删除。替换的命令在下面的<a href="/sed/#u6539">改</a>中会详细介绍。</p>
<h2 id="u6539"><a href="#u6539" class="headerlink" title="改"></a>改</h2><h3 id="u6240_u6709_u884C_u5934/_u5C3E_u589E_u52A0_u9879_u76EE"><a href="#u6240_u6709_u884C_u5934/_u5C3E_u589E_u52A0_u9879_u76EE" class="headerlink" title="所有行头/尾增加项目"></a>所有行头/尾增加项目</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'s/^/China,/'</span> staff.csv</span><br><span class="line">cat staff.csv</span><br></pre></td></tr></table></figure>
<p>其中的<code>s</code>表示替换（substitute），<code>^</code>表示开头，相对应的<code>$</code>表示结尾。</p>
<h3 id="u6240_u6709_u884C_u4FEE_u6539_u9879_u76EE"><a href="#u6240_u6709_u884C_u4FEE_u6539_u9879_u76EE" class="headerlink" title="所有行修改项目"></a>所有行修改项目</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'s/China/US/'</span> staff.csv</span><br><span class="line">cat staff.csv</span><br></pre></td></tr></table></figure>
<p>这样就能把所有的China换成US。如果想把所有的名字后面都加上一个<code>-dev</code>呢？运行：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/,[A-Z][a-z]*/&amp;-dev/'</span> staff.csv</span><br><span class="line">sed -r <span class="string">'s/(,.*),/\1-dev,/'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>其中第一条命令的正则表达式<code>,[A-Z][a-z]*</code>匹配逗号和名字，<code>&amp;</code>表示匹配上的内容，比如对于第二行来说，是<code>,Gavo</code>。第二条命令略微麻烦点，<code>-r</code>表示扩展的正则表达式（extended regular expressions），圆括号表示分组，第一个圆括号中间是第一组，替换的时候用<code>\1</code>表示匹配上的内容。所以<code>\1</code>就是<code>,</code>之前的文本。文件的每一行都有两个逗号，sed会匹配最远的那一个。比如对于第二行来说，匹配到了第二个逗号，所以<code>\1</code>的值就是<code>US,Gavo</code>。加完<code>-dev</code>之后要再补上逗号。所以sed是非常灵活的，可以用多种办法来实现一个功能。</p>
<h3 id="u6B63_u5219_u66FF_u6362"><a href="#u6B63_u5219_u66FF_u6362" class="headerlink" title="正则替换"></a>正则替换</h3><p>给所有的项目都加上引号：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -r <span class="string">'s/[^,]+/"&amp;"/g'</span> staff.csv</span><br><span class="line">sed <span class="operator">-e</span> <span class="string">'s/^\|$/"/g'</span> <span class="operator">-e</span> <span class="string">'s/,/","/g'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>上面的命令是实现的两种方式。第一条命令的意思是除了逗号以外的所有匹配文本都加双引号。对于第二行来说，匹配到了三个文本：<code>US</code>，<code>Gavo</code>和<code>35</code>。第二条命令的思路则完全不同，是先在首尾都加上双引号，然后再把所有的<code>,</code>都替换成<code>&quot;,&quot;</code>。中间的竖线<code>|</code>用反斜杠转义后就是正则表达式中的“或”的意思。</p>
<h3 id="u5168_u5C40_u66FF_u6362"><a href="#u5168_u5C40_u66FF_u6362" class="headerlink" title="全局替换"></a>全局替换</h3><p>把所有的<code>l</code>改成<code>L</code>：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/l/L/g'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>后面的<code>/g</code>代表整行范围内的所有匹配全部替换，不加<code>g</code>的话就会被替换成<code>BiLl</code>。可以换成<code>2</code>只替换第二个匹配项。还可以选择<code>i</code>来忽略大小写，也可以一起用。它们都是正则表达式的范畴。</p>
<p>同时替换<code>l</code>和<code>m</code>，以下两种方式都可以：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/l/L/g; s/m/M/g'</span> staff.csv</span><br><span class="line">sed <span class="operator">-e</span> <span class="string">'s/l/L/g'</span> <span class="operator">-e</span> <span class="string">'s/m/M/g'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<h3 id="u5927_u5C0F_u5199_u66FF_u6362"><a href="#u5927_u5C0F_u5199_u66FF_u6362" class="headerlink" title="大小写替换"></a>大小写替换</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/.*/\L&amp;/'</span> staff.csv</span><br><span class="line">sed <span class="string">'s/.*/\U&amp;/'</span> staff.csv</span><br></pre></td></tr></table></figure>
<p><code>\L</code>就是全部小写（lowercase），<code>\U</code>就是全部大写（uppercase）。<code>&amp;</code>在上文有提到，表示匹配上的内容。</p>
<h3 id="u4FEE_u6539_u6307_u5B9A_u884C"><a href="#u4FEE_u6539_u6307_u5B9A_u884C" class="headerlink" title="修改指定行"></a>修改指定行</h3><p>现在表头的第一列也成了US，把它改成Country：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'1s/US/Country/'</span> staff.csv</span><br><span class="line">cat staff.csv</span><br></pre></td></tr></table></figure></p>
<p>把第2行到4行的US替换成China：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'2,4s/US/China/'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>把第3行整行替换掉：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'3s/.*/China,Hetty,29/'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<h3 id="u5220_u9664_u6240_u6709_u7B26_u53F7"><a href="#u5220_u9664_u6240_u6709_u7B26_u53F7" class="headerlink" title="删除所有符号"></a>删除所有符号</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/[[:punct:]]//g'</span> staff.csv</span><br></pre></td></tr></table></figure>
<p><code>[[:punct:]]</code>是正则表达式中预先定义的子字符类（character classes），代表所有的标点符号。sed支持的<a href="http://www.gnu.org/software/grep/manual/html_node/Character-Classes-and-Bracket-Expressions.html" target="_blank" rel="external">子字符类</a>如下：</p>
<ul>
<li>[:alnum:]：[0-9A-Za-z]</li>
<li>[:alpha:]：[A-Za-z]</li>
<li>[:blank:]：空格和TAB</li>
<li>[:cntrl:]：控制字符（Control characters），ASCII码为000~037和177 (DEL)</li>
<li>[:digit:]：[0-9]</li>
<li>[:graph:]：[:alnum:]和[:punct:]</li>
<li>[:lower:]：[a-z]</li>
<li>[:print:]：[:alnum:]、[:punct:]和空格</li>
<li>[:punct:]：符号 ! “ # $ % &amp; ‘ ( ) * + , - . / : ; &lt; = &gt; ? @ [ \ ] ^ _ ` { | } ~</li>
<li>[:space:]：[:blank:]和回车、换行等</li>
<li>[:upper:]：[A-Z]</li>
<li>[:xdigit:]：16进制 [0-9A-Fa-f]</li>
</ul>
<h2 id="u67E5"><a href="#u67E5" class="headerlink" title="查"></a>查</h2><h3 id="u67E5_u770B_u7279_u5B9A_u884C"><a href="#u67E5_u770B_u7279_u5B9A_u884C" class="headerlink" title="查看特定行"></a>查看特定行</h3><p>查（print）也和<a href="/sed/#u589E">増</a>很类似，区别是把<code>i</code>或<code>a</code>换成<code>p</code>即可：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'1p'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>但是…只是把匹配的行多打一遍而已。如果想要达到<code>grep</code>般的效果，加上<code>-n</code>就可以了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">'1p'</span> staff.csv         <span class="comment"># 查看第一行</span></span><br><span class="line">sed -n <span class="string">'$p'</span> staff.csv         <span class="comment"># 查看最后一行</span></span><br><span class="line">sed -n <span class="string">'/Jane/p'</span> staff.csv    <span class="comment"># 查看包含Jane的一行</span></span><br><span class="line">sed -n <span class="string">'/21/p'</span> staff.csv      <span class="comment"># 查看包含21的一行</span></span><br><span class="line">sed -n <span class="string">'/J/p'</span> staff.csv       <span class="comment"># 查看包含J的两行</span></span><br><span class="line">sed -n <span class="string">'/21$/p'</span> staff.csv     <span class="comment"># 查看以21结尾的一行</span></span><br><span class="line">sed -n <span class="string">'/21/!p'</span> staff.csv     <span class="comment"># 查看包含21以外的其它行</span></span><br></pre></td></tr></table></figure></p>
<p>倒数第二个命令中的<code>21$</code>表示以21结尾。如果要以21开头，用<code>^21</code>。最后一个命令中的<code>!</code>是取反的意思，所以21的记录就反而被隐藏了，而其他的记录倒都显示出来了。</p>
<h3 id="u67E5_u770B_u884C_u8303_u56F4"><a href="#u67E5_u770B_u884C_u8303_u56F4" class="headerlink" title="查看行范围"></a>查看行范围</h3><p>如果想要查看直到匹配某条记录，用下面这条命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Jane/q'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>其中的<code>q</code>代表查到后退出（quit）。还有几种方式：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">'1,/Jane/p'</span> staff.csv         <span class="comment"># 从第一行开始到匹配Jane的记录为止</span></span><br><span class="line">sed -n <span class="string">'/Gavo/,/Jane/p'</span> staff.csv    <span class="comment"># 从匹配Gavo的记录开始到匹配Jane的记录为止</span></span><br><span class="line">sed -n <span class="string">'/Jane/,$p'</span> staff.csv         <span class="comment"># 从匹配Jane的记录开始到最后一行为止</span></span><br></pre></td></tr></table></figure></p>
<h3 id="u67E5_u770B_u5947/_u5076_u6570_u884C"><a href="#u67E5_u770B_u5947/_u5076_u6570_u884C" class="headerlink" title="查看奇/偶数行"></a>查看奇/偶数行</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'n;d'</span> staff.csv       <span class="comment"># 奇数行</span></span><br><span class="line">sed <span class="string">'1d;n;d'</span> staff.csv    <span class="comment"># 偶数行</span></span><br></pre></td></tr></table></figure>
<p>第一条命令中的<code>n;</code>表示输出当前行并立即读取下一行。第二条命令先把第一行记录删除，于是再输出的奇数行就自然变成原来的偶数行了。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这次重温一下Linux下一个很老（反正比我老）很有用的流编辑器：sed（stream editor）。要是不经常使用，很容易忘记。可以把本文当成一个例子库，有用的时候来查一下。<br>]]>
    
    </summary>
    
      <category term="sed" scheme="http://qinghua.github.io/tags/sed/"/>
    
      <category term="linux/unix" scheme="http://qinghua.github.io/categories/linux-unix/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[SaltStack环境安装及入门]]></title>
    <link href="http://qinghua.github.io/saltstack/"/>
    <id>http://qinghua.github.io/saltstack/</id>
    <published>2016-02-23T11:48:11.000Z</published>
    <updated>2016-02-24T02:55:53.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://saltstack.com/" target="_blank" rel="external">SaltStack</a>简称salt，是一个配置管理工具，类似<a href="http://www.ansible.com/get-started" target="_blank" rel="external">Ansible</a>、<a href="https://www.chef.io/chef/" target="_blank" rel="external">Chef</a>和<a href="https://puppetlabs.com/" target="_blank" rel="external">Puppet</a>，可以用脚本批量操作多台机器。SaltStack运行得很快，可以很容易管理上万台服务器，还有<a href="http://docs.saltstack.cn/zh_CN/latest/" target="_blank" rel="external">部分中文文档</a>。它分为服务器（master）和客户端（minion），服务器也是一个客户端。<a href="http://ohmystack.com/articles/salt-1-basic/" target="_blank" rel="external">Salt (1) 入门</a>是个不错的参考教程。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init ubuntu/trusty64</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>master</strong>，它的IP是<strong>192.168.33.17</strong>；另两台叫做<strong>minion1</strong>和<strong>minion2</strong>，它们的IP是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>，其中minion2安装CentOS，其它安装Ubuntu。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"master"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"master"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"minion1"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"minion1"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"minion2"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.box = <span class="string">"bento/centos-7.1"</span></span><br><span class="line">  host.vm.hostname = <span class="string">"minion2"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>改好之后，分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh master</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh minion1</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh minion2</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>首先在master的虚拟机上安装salt-master和salt-minion，注意master自己也是一个minion：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget -O - https://repo.saltstack.com/apt/ubuntu/<span class="number">14.04</span>/amd64/latest/SALTSTACK-GPG-KEY.pub | sudo apt-key add -</span><br><span class="line">sudo sh -c <span class="string">"echo 'deb http://repo.saltstack.com/apt/ubuntu/14.04/amd64/latest trusty main' &gt;&gt; /etc/apt/sources.list"</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y salt-minion</span><br><span class="line">sudo apt-get install -y salt-master</span><br></pre></td></tr></table></figure></p>
<p>然后在minion上安装salt-minion，这次salt-master就没有必要了，Ubuntu和CentOS的安装方法不太一样：<br><figure class="highlight sh"><figcaption><span>minion1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -O - https://repo.saltstack.com/apt/ubuntu/<span class="number">14.04</span>/amd64/latest/SALTSTACK-GPG-KEY.pub | sudo apt-key add -</span><br><span class="line">sudo sh -c <span class="string">"echo 'deb http://repo.saltstack.com/apt/ubuntu/14.04/amd64/latest trusty main' &gt;&gt; /etc/apt/sources.list"</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y salt-minion</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>minion2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm --import https://repo.saltstack.com/yum/redhat/<span class="number">7</span>/x86_64/latest/SALTSTACK-GPG-KEY.pub</span><br><span class="line">sudo sh -c <span class="string">"cat &lt;&lt; EOF &gt;/etc/yum.repos.d/saltstack.repo</span><br><span class="line">[saltstack-repo]</span><br><span class="line">name=SaltStack repo for RHEL/CentOS 7</span><br><span class="line">baseurl=https://repo.saltstack.com/yum/redhat/7/x86_64/latest</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://repo.saltstack.com/yum/redhat/7/x86_64/latest/SALTSTACK-GPG-KEY.pub</span><br><span class="line">EOF"</span></span><br><span class="line">sudo yum clean expire-cache</span><br><span class="line">sudo yum update</span><br><span class="line">sudo yum install -y salt-minion</span><br></pre></td></tr></table></figure>
<p>默认安装好的minion会自动试图连接到名为salt的master去，所以我们得配置一下，然后重新启动salt-minion服务：<br><figure class="highlight sh"><figcaption><span>master and minion1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">"s/#master: salt/master: 192.168.33.17/"</span> /etc/salt/minion</span><br><span class="line">sudo service salt-minion restart</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>minion2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">"s/#master: salt/master: 192.168.33.17/"</span> /etc/salt/minion</span><br><span class="line">sudo systemctl start salt-minion</span><br></pre></td></tr></table></figure>
<p>随便在哪台虚拟机上运行<code>sudo tail -1 /var/log/salt/minion</code>，如果看到错误消息<strong>The Salt Master has cached the public key for this node</strong>，那就说明前面的安装都是顺利的—这是因为第一次运行的时候，需要建立互信。Salt维护着一个互信列表，在master上运行<code>sudo salt-key</code>可以看到这个表，现在应该是这样子的：<br><strong>Accepted Keys:</strong><br><strong>Denied Keys:</strong><br><strong>Unaccepted Keys:</strong><br>master<br>minion1<br>minion2<br><strong>Rejected Keys:</strong></p>
<p>从上表可以看出，互信列表里的记录有<a href="https://docs.saltstack.com/en/latest/ref/cli/salt-key.html#description" target="_blank" rel="external">四种状态</a>：</p>
<ul>
<li>Unaccepted：待处理</li>
<li>Accepted：互信</li>
<li>Rejected：运维人员运行命令拒绝</li>
<li>Denied：master自动拒绝（比如ID重复等）</li>
</ul>
<p>现在所有的minion包括master自己都是处于Unaccepted的状态。运行以下命令就可以把它们都加入到Accepted：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo salt-key --accept=master --yes</span><br><span class="line">sudo salt-key --accept=minion1 --yes</span><br><span class="line">sudo salt-key --accept=minion2 --yes</span><br></pre></td></tr></table></figure></p>
<p>如果minion太多了，也可以用<code>sudo salt-key --accept-all --yes</code>来全部accept。忽略<code>--yes</code>可以让我们手动确认所有的Unaccepted记录。再次运行<code>sudo salt-key</code>，确认所有的minion都已经加入到Accepted Keys里了，安装步骤就此完成。</p>
<p>小贴士：可以用<code>sudo salt-key --reject=minion2</code>来把minion2加入到Rejected列表中。用<code>sudo salt-key --include-all --accept-all</code>来把Rejected列表中的minion再加到Accepted中来。</p>
<h2 id="u8FD0_u884C_u547D_u4EE4"><a href="#u8FD0_u884C_u547D_u4EE4" class="headerlink" title="运行命令"></a>运行命令</h2><h3 id="salt"><a href="#salt" class="headerlink" title="salt"></a>salt</h3><p>安装完成之后，在master上运行<code>sudo salt &#39;*&#39; test.ping</code>可以看到各minion是否能联通。其中的<strong>*</strong>代表<a href="https://docs.saltstack.com/en/latest/topics/targeting/index.html" target="_blank" rel="external">目标（target）</a>，这里即是所有的minion（master现在也是一个minion），<strong>test.ping</strong>称为<a href="https://docs.saltstack.com/en/latest/ref/modules/" target="_blank" rel="external">执行模块（execution module）</a>，也就是需要在目标上调用的方法。<a href="https://docs.saltstack.com/en/latest/ref/modules/all/index.html" target="_blank" rel="external">这个列表</a>里记载了所有的原生执行模块。我们来尝试一下其中的<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.cmdmod.html#module-salt.modules.cmdmod" target="_blank" rel="external">cmdmod</a>模块：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo salt minion1 cmd.run <span class="string">'ifconfig'</span></span><br></pre></td></tr></table></figure></p>
<p>这就相当于在minion1上直接运行ifconfig了。</p>
<h3 id="salt-call"><a href="#salt-call" class="headerlink" title="salt-call"></a>salt-call</h3><p>Salt还提供了一个<code>salt-call</code>命令，它只能在本机执行，所以无需输入目标。在master上运行<code>sudo salt-call cmd.run &#39;hostname&#39;</code>，效果相当于直接本地运行命令。有所不同的是，它相当于运行在salt的机制上。Salt需要两个端口来运行：通过<a href="http://zeromq.org/" target="_blank" rel="external">ZeroMQ</a>在4505发消息，4506用来接收结果，所有的minion都会订阅4505端口。也就是说，运行这条命令使用了这两个端口。配合上<code>--log-level=debug</code>的参数，使得<code>salt-call</code>非常适用于调试。运行以下命令：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo salt-call --log-level=debug disk.usage</span><br></pre></td></tr></table></figure></p>
<p>从打印出来的调试信息，我们能看到原来<strong>disk.usage</strong>模块用的是<code>df -P</code>命令。</p>
<h3 id="salt-run"><a href="#salt-run" class="headerlink" title="salt-run"></a>salt-run</h3><p>最后再介绍一个<code>salt-run</code>命令。我们简单地试一试在master上运行<code>sudo salt-run manage.up</code>，这个manage.up是个runner，在这里它的作用类似于test.ping模块，也是查看所有minion的状态，但是不需要指定目标即可使用。它一般分为几个步骤，一个步骤内并行执行，步骤之间串行执行。比如先部署好数据库（步骤A）再部署应用服务器（步骤B）。<code>salt-run</code>运行的命令称为runner，<a href="https://docs.saltstack.com/en/latest/ref/runners/all/index.html" target="_blank" rel="external">这个列表</a>里记载了所有的原生runner。</p>
<h2 id="u6307_u5B9A_u76EE_u6807"><a href="#u6307_u5B9A_u76EE_u6807" class="headerlink" title="指定目标"></a>指定目标</h2><p>Salt支持多种方式来指定目标，简单尝试一下就知道啦。<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo salt <span class="string">'minion*'</span> test.ping                      <span class="comment"># 通配符，以"minion"开头的minion</span></span><br><span class="line">sudo salt -L <span class="string">'minion1,minion2'</span> test.ping           <span class="comment"># 列表，minion1和minion2</span></span><br><span class="line">sudo salt -E <span class="string">'minion(1|2)'</span> test.ping               <span class="comment"># 正则表达式，minion1或minion2</span></span><br><span class="line">sudo salt -S <span class="string">'192.168.33.19'</span> test.ping             <span class="comment"># IP，minion2</span></span><br><span class="line">sudo salt -G <span class="string">'os:Ubuntu'</span> test.ping                 <span class="comment"># Grains，操作系统为Ubuntu</span></span><br><span class="line">sudo salt -C <span class="string">'minion* and G@os:Ubuntu'</span> test.ping   <span class="comment"># 组合，以"minion"开头的minion并且操作系统为Ubuntu</span></span><br><span class="line">sudo salt -C <span class="string">'master or G@os:CentOS'</span> test.ping     <span class="comment"># 组合，master或操作系统为CentOS（不区分大小写）</span></span><br><span class="line">sudo salt -I <span class="string">'region:cn'</span> test.ping                 <span class="comment"># Pillar，region为cn的minion</span></span><br></pre></td></tr></table></figure></p>
<p>其中的<a href="https://docs.saltstack.com/en/latest/topics/targeting/grains.html" target="_blank" rel="external">Grains</a>是minion的属性，包含机器名、IP、操作系统、CPU等多种信息。可以运行<code>sudo salt &#39;*&#39; grains.items</code>来查看所有的grains数据。那<a href="https://docs.saltstack.com/en/latest/topics/pillar/index.html" target="_blank" rel="external">Pillar</a>又是什么鬼？简言之，Pillar是存放在master的变量，Grains是minion自己的常量。上面的命令中，Pillar这行应该会报错：<strong>No minions matched the target</strong>，这是因为我们没有在master上加Pillar的缘故。Pillar文件为yaml格式，默认存放在<code>/srv/pillar</code>里。由于pillar只会加密传送给指定的minion，所以可以存放密码等敏感信息。我们来加两个文件：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">"cat &lt;&lt; EOF &gt; /srv/pillar/top.sls</span><br><span class="line">base:</span><br><span class="line">  '*':</span><br><span class="line">    - default</span><br><span class="line">EOF"</span></span><br><span class="line">sudo sh -c <span class="string">"echo 'region: cn' &gt; /srv/pillar/default.sls"</span></span><br></pre></td></tr></table></figure></p>
<p>首先创建的<code>top.sls</code>是一个默认的入口文件，它表示所有minion都适用<code>default.sls</code>文件。而<code>default.sls</code>里指定了变量。运行以下的命令：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo salt <span class="string">'*'</span> pillar.items            <span class="comment"># 生成、分发、查看现在的pillar。用pillar.raw可以仅查看当前值</span></span><br><span class="line">sudo salt -I <span class="string">'region:cn'</span> test.ping    <span class="comment"># 再次运行test.ping，以pillar为目标</span></span><br></pre></td></tr></table></figure></p>
<p>就能看到现在已经不报错了，所有的minion都是目标。</p>
<h2 id="u7EF4_u6301_u72B6_u6001_uFF08State_uFF09"><a href="#u7EF4_u6301_u72B6_u6001_uFF08State_uFF09" class="headerlink" title="维持状态（State）"></a>维持状态（State）</h2><p>Salt提供了<a href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html" target="_blank" rel="external">state</a>（点开一看，设计哲学是“简单，简单，简单”。大家都知道重要的事情说三遍）的方式让我们维持所有minion的状态一致。我们来加两个文件：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">"cat &lt;&lt; EOF &gt; /srv/salt/top.sls</span><br><span class="line">base:</span><br><span class="line">  '*':</span><br><span class="line">    - default</span><br><span class="line">EOF"</span></span><br><span class="line">sudo sh -c <span class="string">"cat &lt;&lt; EOF &gt; /srv/salt/default.sls</span><br><span class="line">/tmp/ggg:</span><br><span class="line">  file.directory:</span><br><span class="line">    - makedirs: True</span><br><span class="line">EOF"</span></span><br></pre></td></tr></table></figure></p>
<p>如同pillar，首先创建的<code>top.sls</code>是一个默认的入口文件，<code>default.sls</code>里指定了这个状态需要有<code>/tmp/ggg</code>这个文件夹。接下来让我们来运行它：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo salt <span class="string">'master'</span> state.show_sls default    <span class="comment"># 检查并显示default的state</span></span><br><span class="line">sudo salt <span class="string">'master'</span> state.sls default         <span class="comment"># 运行default的state</span></span><br><span class="line">ls /tmp                                      <span class="comment"># 能够看到ggg文件夹已经创建好了</span></span><br><span class="line">sudo salt <span class="string">'master'</span> state.sls default         <span class="comment"># 再次运行default的state</span></span><br><span class="line">ls /tmp                                      <span class="comment"># 运行几次都一样，维持状态</span></span><br></pre></td></tr></table></figure></p>
<p>State可以说是salt的核心功能。它通过yaml格式的数据文件sls（<strong>S</strong>a<strong>L</strong>t <strong>S</strong>tate file）确保了一个系统应该是什么状态的。<a href="https://docs.saltstack.com/en/latest/ref/states/all/" target="_blank" rel="external">这个列表</a>里记载了所有可用的状态。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://saltstack.com/">SaltStack</a>简称salt，是一个配置管理工具，类似<a href="http://www.ansible.com/get-started">Ansible</a>、<a href="https://www.chef.io/chef/">Chef</a>和<a href="https://puppetlabs.com/">Puppet</a>，可以用脚本批量操作多台机器。SaltStack运行得很快，可以很容易管理上万台服务器，还有<a href="http://docs.saltstack.cn/zh_CN/latest/">部分中文文档</a>。它分为服务器（master）和客户端（minion），服务器也是一个客户端。<a href="http://ohmystack.com/articles/salt-1-basic/">Salt (1) 入门</a>是个不错的参考教程。<br>]]>
    
    </summary>
    
      <category term="salt" scheme="http://qinghua.github.io/tags/salt/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[用容器轻松搭建Bamboo来提供Marathon的对外服务]]></title>
    <link href="http://qinghua.github.io/marathon-bamboo/"/>
    <id>http://qinghua.github.io/marathon-bamboo/</id>
    <published>2016-02-22T11:26:11.000Z</published>
    <updated>2016-02-22T07:04:24.000Z</updated>
    <content type="html"><![CDATA[<p>如果我们在marathon上部署了一个tomcat服务并希望它能暴露给外网，应该怎么做呢？<a href="https://github.com/QubitProducts/bamboo" target="_blank" rel="external">Bamboo</a>提供了一个非常方便运行的办法帮我们做到这一点。它集成了HAproxy，当marathon检测到应用挂掉并重启应用时，bamboo能够检测到并更新HAproxy的配置文件，然后自动重启HAproxy，从而无须人工干预便能持续不断地对外提供服务。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>参考《用容器轻松搭建Marathon运行环境》的<a href="/mesos-marathon/#u51C6_u5907_u5DE5_u4F5C">准备工作</a>一节，用vagrant搭建两台虚拟机<strong>master</strong>和<strong>slave</strong>。</p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>参考《用容器轻松搭建Marathon运行环境》的<a href="/mesos-marathon/#u642D_u5EFA_u73AF_u5883">搭建环境</a>一节，但是用下面这个命令来启动marathon。它相比原来的命令多了一个<code>--event_subscriber http_callback</code>的参数，如果不配置，便不能实现HAproxy动态加载服务的功能。<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=ma \</span><br><span class="line">    mesosphere/marathon:v0.<span class="number">15.0</span> \</span><br><span class="line">    --master zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/mesos \</span><br><span class="line">    --zk zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/marathon \</span><br><span class="line">    --event_subscriber http_callback</span><br></pre></td></tr></table></figure></p>
<p>现在启动Bamboo镜像，在里面指定marathon的地址：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">8000</span>:<span class="number">8000</span> \</span><br><span class="line">    -p <span class="number">80</span>:<span class="number">80</span> \</span><br><span class="line">    --name=bam \</span><br><span class="line">    <span class="operator">-e</span> MARATHON_ENDPOINT=http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span> \</span><br><span class="line">    <span class="operator">-e</span> BAMBOO_ENDPOINT=http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8000</span> \</span><br><span class="line">    <span class="operator">-e</span> BAMBOO_ZK_HOST=<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span> \</span><br><span class="line">    <span class="operator">-e</span> BAMBOO_ZK_PATH=/bamboo \</span><br><span class="line">    <span class="operator">-e</span> BIND=<span class="string">":8000"</span> \</span><br><span class="line">    <span class="operator">-e</span> CONFIG_PATH=<span class="string">"config/production.example.json"</span> \</span><br><span class="line">    <span class="operator">-e</span> BAMBOO_DOCKER_AUTO_HOST=<span class="literal">true</span> \</span><br><span class="line">    gregory90/bamboo:<span class="number">0.2</span>.<span class="number">11</span></span><br></pre></td></tr></table></figure></p>
<p>在浏览器打开<code>http://192.168.33.18:8000/</code>应该能看到下图：<br><img src="/img/bamboo.png" alt=""></p>
<h2 id="u670D_u52A1_u53D1_u73B0"><a href="#u670D_u52A1_u53D1_u73B0" class="headerlink" title="服务发现"></a>服务发现</h2><p>现在让我们用marathon来启动一个tomcat服务。在任意一台机器上运行以下命令，把创建tomcat服务的请求发送给marathon的REST api：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span>/v2/apps \</span><br><span class="line">    -H <span class="string">"Content-type: application/json"</span> \</span><br><span class="line">    <span class="operator">-d</span> <span class="string">'&#123;"cpus":0.5,"mem":200,"disk":0,"instances":1,"id":"tomcat", </span><br><span class="line">    "container":&#123;"docker":&#123;"image":"tomcat","network":"BRIDGE","portMappings": </span><br><span class="line">    [&#123;"containerPort":8080,"hostPort":0,"servicePort":0,"protocol":"tcp"&#125;]&#125;&#125;&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>分别刷新marathon和bamboo，就能看到它们各自多了个tomcat的服务。点击bamboo页面上的<strong>/tomcat</strong>记录最右边的加号按钮，在<strong>acl</strong>里输入<code>path_beg -i /</code>（表示运行在根目录上，有兴趣的话可以参考HAproxy的<a href="http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#7" target="_blank" rel="external">ACL语法</a>），然后点击<strong>Create</strong>按钮：<br><img src="/img/bamboo-tomcat.png" alt=""></p>
<p>顺利的话，打开<code>http://192.168.33.18/</code>应该能看到tomcat出现啦：<br><img src="/img/tomcat.jpg" alt=""></p>
<p>这个时候，在marathon的页面上点击tomcat这行记录，便会到tomcat application页面里。如下图选中当前的tomcat实例，点击<strong>Kill</strong>按钮：<br><img src="/img/marathon-application-tomcat.png" alt=""></p>
<p>稍等几秒，就会看到tomcat的服务运行地址从<code>192.168.33.19:31071</code>变成了<code>192.168.33.19:31571</code>，端口因机而异。再回去刷新tomcat的<code>http://192.168.33.18/</code>页面，是不是仍然提供服务呢？如果你手快，应该能看到<strong>503 Service Unavailable</strong>，那就多刷新两下 ：） 到slave虚拟机上用命令删除tomcat容器，再观察一下，是不是一样的效果呢？</p>
<h2 id="u5176_u4ED6_u65B9_u6CD5"><a href="#u5176_u4ED6_u65B9_u6CD5" class="headerlink" title="其他方法"></a>其他方法</h2><p>Marathon官方还支持其它<a href="https://mesosphere.github.io/marathon/docs/service-discovery-load-balancing" target="_blank" rel="external">三种服务发现的方法</a>：</p>
<ol>
<li>Mesos-DNS：Mesosphere公司提供的DNS产品，不仅适用于marathon，而且适用于其它Mesos Framework。</li>
<li>Marathon-lb：感觉上跟k8s的<a href="http://kubernetes.io/v1.1/docs/user-guide/services.html#type-nodeport" target="_blank" rel="external">NodePort</a>有点像，不过它像Bamboo那样包含了HAproxy。</li>
<li>haproxy-marathon-bridge：现在已经不推荐了。需要在每个slave上安装HAproxy，定时更新HAproxy的配置文件。</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>如果我们在marathon上部署了一个tomcat服务并希望它能暴露给外网，应该怎么做呢？<a href="https://github.com/QubitProducts/bamboo">Bamboo</a>提供了一个非常方便运行的办法帮我们做到这一点。它集成了HAproxy，当marathon检测到应用挂掉并重启应用时，bamboo能够检测到并更新HAproxy的配置文件，然后自动重启HAproxy，从而无须人工干预便能持续不断地对外提供服务。<br>]]>
    
    </summary>
    
      <category term="bamboo" scheme="http://qinghua.github.io/tags/bamboo/"/>
    
      <category term="marathon" scheme="http://qinghua.github.io/tags/marathon/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[轻松搭建Docker Swarm运行环境]]></title>
    <link href="http://qinghua.github.io/docker-swarm/"/>
    <id>http://qinghua.github.io/docker-swarm/</id>
    <published>2016-02-18T13:19:50.000Z</published>
    <updated>2016-02-18T13:19:53.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://docs.docker.com/swarm/" target="_blank" rel="external">Docker Swarm</a>是官方发布的集群容器管理工具。它的特点是：比较轻量级，无缝支持标准的docker API。<a href="http://blog.daocloud.io/swarm_analysis_part1/" target="_blank" rel="external">深入浅出Swarm</a>一文很清晰地讲解了它的架构和命令。本文从零开始搭建并管理一个swarm集群。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>manager</strong>，它的IP是<strong>192.168.33.17</strong>；另两台叫做<strong>node1</strong>和<strong>node2</strong>，它们的IP是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"manager"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"manager"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"node1"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"node1"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"node2"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"node2"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh manager</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node1</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node2</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>想要让swarm管理node，首先得让docker daemon支持TCP。在三台虚拟机上运行以下命令：<br><figure class="highlight sh"><figcaption><span>manager and node1 and node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo rm /etc/docker/key.json    <span class="comment"># 免得我们用vagrant生成的docker id都一样，删掉了重启docker服务会自动生成一个新的</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<p>接下来，我们用最简单的静态节点列表方式来启动swarm环境。把node1和node2的节点信息都写到参数里即可：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -t -p <span class="number">2376</span>:<span class="number">2375</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> manage nodes://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2375</span>,<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">2375</span></span><br></pre></td></tr></table></figure></p>
<p>2376是我随便设的一个端口，可以改成2375外的其他可用端口，因为2375已经被docker daemon占用了。可以Ctrl+C后，用下面这个命令查看：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo netstat -tulnp | grep <span class="number">2375</span></span><br></pre></td></tr></table></figure></p>
<p>随便在哪台机器运行以下命令就能看到这个集群的信息和节点信息。这里的2376就是上面随便设出来的2376：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> info</span><br></pre></td></tr></table></figure></p>
<h2 id="u8FD0_u884C_u5BB9_u5668"><a href="#u8FD0_u884C_u5BB9_u5668" class="headerlink" title="运行容器"></a>运行容器</h2><p>Swarm的环境已经搭建完成，现在我们可以用swarm来运行容器了。随便在哪台机器运行以下命令来创建一个busybox容器：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> run <span class="operator">-d</span> busybox sleep <span class="number">3000</span></span><br></pre></td></tr></table></figure></p>
<p>可能一开始的时候有点儿慢，这是因为需要下载镜像的缘故。如果等不及，就到两个node里分别先把镜像下载下来：<code>docker pull busybox</code>。之所以说无缝支持docker API，那是因为swarm里能运行所有的docker命令。跑一下<code>docker -H tcp://192.168.33.17:2376 ps</code>就能看到一个busybox的容器已经启动起来了。容器的<strong>NAMES</strong>属性里有node的信息，所以不需要分别在两个node运行<code>docker ps</code>就能看到这个busybox的容器在哪个node运行。之后，再运行3次上面的命令，共创建4个busybox的容器，就能看到它们被均匀分配到两个node上了。这是因为swarm默认的调度策略所致。目前swarm支持<a href="https://docs.docker.com/swarm/scheduler/strategy/" target="_blank" rel="external">三种调度策略</a>：</p>
<ul>
<li>spread：默认，swarm会把任务分配到目前运行的容器数量最少的node上去。这里说的容器数量包括已经停止的容器。</li>
<li>binpack：把任务分配到目前最大负荷的node上去。目的是把其他机器的资源留给将来可能要运行的大容器。</li>
<li>random：随机分配。</li>
</ul>
<p>如果看到的容器分配不均匀，那很可能是存在着非运行中的容器，可以用<code>docker ps -a</code>看一下。如果想要修改调度策略，可以在manager启动的时候指定<code>--strategy</code>参数，比如修改成binpack：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> `docker ps -aq`</span><br><span class="line">docker run -t -p <span class="number">2376</span>:<span class="number">2375</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> manage nodes://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2375</span>,<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">2375</span> --strategy binpack</span><br></pre></td></tr></table></figure></p>
<p>这回再试试启动4个新的busybox，是不是都跑到同一个node上去了？Swarm的过滤器功能还允许我们指定让容器运行在哪个node上。目前，<a href="https://docs.docker.com/swarm/scheduler/filter/" target="_blank" rel="external">swarm支持如下的过滤器</a>：</p>
<ul>
<li>node过滤器<ul>
<li>constraint：限制新任务只能在label符合的node上执行</li>
<li>health：限制新任务只能在“健康”的node上执行</li>
</ul>
</li>
<li>容器过滤器<ul>
<li>affinity：使新任务在已运行某个名字或label的容器，或者有某个镜像的node上执行</li>
<li>dependency：使新任务在有依赖（–volumes-from=dependency、–link=dependency:alias或–net=container:dependency）的node上执行</li>
<li>port：使新任务在某个端口可用的node上执行</li>
</ul>
</li>
</ul>
<p>可以在manager启动的时候指定<code>--filter</code>参数来启用过滤器功能。我们先来试验一下constraint。由于Label是docker daemon的属性，所以我们又要修改<code>/etc/default/docker</code>并重启docker daemon。假设node1为ssd，node2为普通disk：<br><figure class="highlight sh"><figcaption><span>node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> `docker ps -aq`</span><br><span class="line">sudo sed -i <span class="string">'$d'</span> /etc/default/docker    <span class="comment"># 删掉最后一行，因为要加新的label</span></span><br><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --label storage=ssd\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> `docker ps -aq`</span><br><span class="line">sudo sed -i <span class="string">'$d'</span> /etc/default/docker    <span class="comment"># 删掉最后一行，因为要加新的label</span></span><br><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --label storage=disk\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure>
<p>随便在哪台机器运行以下命令，看看两个node是不是分别多出来<code>storage==ssd</code>和<code>storage==disk</code>：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> info</span><br></pre></td></tr></table></figure></p>
<p>如果不是，就重启一下manager的swarm容器。然后就可以指定<code>storage=ssd</code>的node运行：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> run <span class="operator">-d</span> <span class="operator">-e</span> constraint:storage==ssd busybox sleep <span class="number">3000</span></span><br></pre></td></tr></table></figure></p>
<p>随便再创建几个容器玩玩，再换<code>constraint:storage==disk</code>试试看。熟悉之后，我们再试验一下affinity：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> run <span class="operator">-d</span> <span class="operator">-e</span> constraint:storage==disk --name=bb busybox sleep <span class="number">3000</span></span><br></pre></td></tr></table></figure></p>
<p>要的就是<strong>bb</strong>这个名字。然后运行以下命令让新容器运行在有bb容器的node上，也就是node2：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> run <span class="operator">-d</span> <span class="operator">-e</span> affinity:container==bb busybox sleep <span class="number">3000</span></span><br></pre></td></tr></table></figure></p>
<p>在node2上运行<code>docker ps</code>应该能看到新容器已经启动起来了。如果constraint和affinity冲突会怎样呢？试试看：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> run <span class="operator">-d</span>  <span class="operator">-e</span> affinity:container==bb <span class="operator">-e</span> constraint:storage==ssd busybox sleep <span class="number">3000</span></span><br></pre></td></tr></table></figure></p>
<p>不出意外的话，应该能看见<strong>unable to find a node that satisfies storage==ssd</strong>的错误消息了吧。</p>
<h2 id="u4E3B_u673A_u53D1_u73B0"><a href="#u4E3B_u673A_u53D1_u73B0" class="headerlink" title="主机发现"></a>主机发现</h2><p>Swarm共支持下面<a href="https://docs.docker.com/swarm/discovery/" target="_blank" rel="external">几种主机发现方式</a>：</p>
<ul>
<li>分布式键值存储<ul>
<li>Consul 0.5.1或更高版本</li>
<li>Etcd 2.0或更高版本</li>
<li>ZooKeeper 3.4.5或更高版本</li>
</ul>
</li>
<li>静态方式<ul>
<li>文件</li>
<li>节点列表</li>
</ul>
</li>
<li>Docker Hub</li>
</ul>
<p>我们刚才<a href="/docker-swarm/#u642D_u5EFA_u73AF_u5883">搭建环境</a>用到的是静态的节点列表方式，现在我们再试试其它几种方式。</p>
<h3 id="u6587_u4EF6"><a href="#u6587_u4EF6" class="headerlink" title="文件"></a>文件</h3><p>先从简单的开始。文件的主机发现方式和节点列表很类似，它们都是静态的。首先把node的信息都写到一个临时文件<code>/tmp/cluster</code>里，然后让swamp容器管理这个文件即可：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="number">192.168</span>.<span class="number">33</span>.[<span class="number">18</span>:<span class="number">19</span>]:<span class="number">2375</span> &gt; /tmp/cluster</span><br><span class="line">docker run -t -p <span class="number">2376</span>:<span class="number">2375</span> -v /tmp/cluster:/tmp/cluster swarm:<span class="number">1.1</span>.<span class="number">0</span> manage file:///tmp/cluster</span><br></pre></td></tr></table></figure></p>
<p>随便选台虚拟机检查一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> info</span><br></pre></td></tr></table></figure></p>
<p>是不是有换汤不换药的感觉？</p>
<h3 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h3><p>接下来试验一下ZooKeeper。先在manager上启动一个ZooKeeper的服务：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=zk \</span><br><span class="line">    <span class="operator">-e</span> MYID=<span class="number">1</span> \</span><br><span class="line">    <span class="operator">-e</span> SERVERS=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    mesoscloud/zookeeper:<span class="number">3.4</span>.<span class="number">6</span>-ubuntu-<span class="number">14.04</span></span><br></pre></td></tr></table></figure></p>
<p>然后运行swarm manager：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -t -p <span class="number">2376</span>:<span class="number">2375</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> manage zk://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2181</span>/swarm</span><br></pre></td></tr></table></figure></p>
<p>由于这回不像文件和节点列表方式那样静态，我们需要把两个node加入到集群里：<br><figure class="highlight sh"><figcaption><span>node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> join --addr=<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2375</span> zk://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2181</span>/swarm</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> join --addr=<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">2375</span> zk://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2181</span>/swarm</span><br></pre></td></tr></table></figure>
<p>随便选台虚拟机检查一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> info    <span class="comment"># 由于是动态加载，可能需要等待半分钟左右才能看见node</span></span><br></pre></td></tr></table></figure></p>
<p>Consul和Etcd也都很类似，这里就不一一列举了。</p>
<h3 id="Docker_Hub"><a href="#Docker_Hub" class="headerlink" title="Docker Hub"></a>Docker Hub</h3><p>Docker Hub的主机发现方式，就是在docker hub上使用发现服务来生成一个唯一的集群ID（别在生产环境上这么干！）。Docker hub会为我们保留大概一个星期。在任意一台机器上运行以下命令：<br><figure class="highlight sh"><figcaption><span>manager or node1 or node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm swarm:<span class="number">1.1</span>.<span class="number">0</span> create</span><br></pre></td></tr></table></figure></p>
<p>然后我们就能看见上面的命令生成了一个字符串，这就是我们的集群ID。在我的机器上是这样的：<code>3137ebf83d771f1db06bf4eab7ccc73b</code>。我大天朝的网络，有时候会出现<strong>TLS handshake timeout</strong>，那就再运行一次吧。</p>
<p>这时候可以把manager启动起来了，别忘了替换成你自己的token：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -t -p <span class="number">2376</span>:<span class="number">2375</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> manage token://<span class="number">3137</span>ebf83d771f1db06bf4eab7ccc73b</span><br></pre></td></tr></table></figure></p>
<p>然后可以在两个node上分别运行以下命令，启动swarm的代理并加入到集群中，别忘了替换成你自己的token：<br><figure class="highlight sh"><figcaption><span>node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> join --addr=<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2375</span> token://<span class="number">3137</span>ebf83d771f1db06bf4eab7ccc73b</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> join --addr=<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">2375</span> token://<span class="number">3137</span>ebf83d771f1db06bf4eab7ccc73b</span><br></pre></td></tr></table></figure>
<p>随便选台虚拟机检查一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> info    <span class="comment"># 可能比较慢，下面那条命令更快</span></span><br><span class="line">docker run --rm swarm:<span class="number">1.1</span>.<span class="number">0</span> list token://<span class="number">3137</span>ebf83d771f1db06bf4eab7ccc73b</span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="https://docs.docker.com/swarm/">Docker Swarm</a>是官方发布的集群容器管理工具。它的特点是：比较轻量级，无缝支持标准的docker API。<a href="http://blog.daocloud.io/swarm_analysis_part1/">深入浅出Swarm</a>一文很清晰地讲解了它的架构和命令。本文从零开始搭建并管理一个swarm集群。<br>]]>
    
    </summary>
    
      <category term="docker" scheme="http://qinghua.github.io/tags/docker/"/>
    
      <category term="swarm" scheme="http://qinghua.github.io/tags/swarm/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[用容器轻松搭建Chronos运行环境]]></title>
    <link href="http://qinghua.github.io/mesos-chronos/"/>
    <id>http://qinghua.github.io/mesos-chronos/</id>
    <published>2016-02-15T13:04:31.000Z</published>
    <updated>2016-02-17T01:28:36.000Z</updated>
    <content type="html"><![CDATA[<p>Apache Mesos把自己定位成一个数据中心操作系统，它能管理上万台的从机（slave）。Framework相当于这个操作系统的应用程序，每当应用程序需要执行，Framework就会在Mesos中选择一台有合适资源（cpu、内存等）的从机来运行。Chronos是Framework的一种，被Airbnb公司设计用来代替cron执行作业。本文尝试从零开始用docker搭建Mesos和Chronos的运行环境，并用此环境运行作业。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>参考《用容器轻松搭建Marathon运行环境》的<a href="/mesos-marathon/#u51C6_u5907_u5DE5_u4F5C">准备工作</a>一节，用vagrant搭建两台虚拟机<strong>master</strong>和<strong>slave</strong>。</p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>参考《用容器轻松搭建Marathon运行环境》的<a href="/mesos-marathon/#u642D_u5EFA_u73AF_u5883">搭建环境</a>一节，跳过marathon部分。</p>
<p>搭建mesos master和slave环境完成后，最后在master的虚拟机上启动chronos：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=ch \</span><br><span class="line">    mesosphere/chronos:chronos-<span class="number">2.4</span>.<span class="number">0</span>-<span class="number">0.1</span>.<span class="number">20150828104228</span>.ubuntu1404-mesos-<span class="number">0.27</span>.<span class="number">0</span>-<span class="number">0.2</span>.<span class="number">190</span>.ubuntu1404 \</span><br><span class="line">    usr/bin/chronos \</span><br><span class="line">    --master zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/mesos \</span><br><span class="line">    --zk_hosts zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/marathon \</span><br><span class="line">    --cluster_name ggg</span><br></pre></td></tr></table></figure></p>
<p>我们还可以打开<code>http://192.168.33.18:8080/</code>感知一下chronos：<br><img src="/img/chronos.jpg" alt=""></p>
<h2 id="u8FD0_u884C_u4F5C_u4E1A"><a href="#u8FD0_u884C_u4F5C_u4E1A" class="headerlink" title="运行作业"></a>运行作业</h2><p>在chronos的页面上，点击<strong>New Job</strong>按钮，然后填入如下参数：</p>
<ul>
<li>NAME：test</li>
<li>COMMAND：docker run -d busybox sleep 30</li>
<li>SCHEDULE P：T1M</li>
</ul>
<p>如下图所示：<br><img src="/img/chronos-run-job.jpg" alt=""></p>
<p>点击<strong>Create</strong>按钮，立刻就能看见有一个名为<strong>test</strong>的作业正在运行。同时，在mesos的主页上也能看到有一个任务运行起来了。如果手慢一点或者喝了杯茶，还能看见有几个任务已经是完成的状态了，这是因为根据我们设置的T1M，1分钟之后，chronos就会帮助再重新启动一次作业。另外还可以在slave的虚拟机上用<code>docker ps -a</code>看到busybox容器已经启动起来了。</p>
<p><em>T1M的意思是：1分钟之后。1M的意思是：1个月之后。1Y2M3DT4H5M6S的意思是：1年2月3天4小时5分钟6秒之后。</em></p>
<h2 id="u7BA1_u7406_u4F5C_u4E1A"><a href="#u7BA1_u7406_u4F5C_u4E1A" class="headerlink" title="管理作业"></a>管理作业</h2><p>新建作业的窗口里还有一个<strong>Other settings</strong>的链接，是可以在里面设置一些高级功能的，比如说CPU、内存和磁盘，默认是0.1、128MB和256MB，还能设置作业优先级、运行方式等。已经生成的作业也可以再次修改、强制运行、复制和删除。作业也可以是一次性的，只要把SCHEDULE R设成0就可以了，记得同时调整一下时间T哦。</p>
<p>作业直接还可以指定依赖。我们再创建一个如下作业：</p>
<ul>
<li>NAME：test2</li>
<li>COMMAND：date &gt;&gt; /tmp/test.txt</li>
<li>PARENTS: test</li>
</ul>
<p>这回我们并没有设置SCHEDULE，而是设置了一个PARENTS为<strong>test</strong>，它的意思就是当test运行成功时，运行这个test2的作业。等1分钟，test作业再次运行后，在slave虚拟机上运行<code>docker exec ms1 cat /tmp/test.txt</code>就能看到当前时间已经被写进<code>/tmp/test.txt</code>文件中，test2作业也被成功运行了。</p>
<p>我们再创建第三个作业：</p>
<ul>
<li>NAME：test3</li>
<li>COMMAND：echo “hello world” &gt;&gt; /tmp/test.txt</li>
<li>PARENTS: test, test2</li>
</ul>
<p>这次test3依赖于test和test2，那么它们之间是“或”还是“和”的关系呢？我们强制运行一次test，在slave虚拟机上运行<code>docker exec ms1 cat /tmp/test.txt</code>就能看到增加了一行日期和一行hello world。强制运行一次test2，却只增加了一行日期而没有增加hello world。由此推断，只有当test2和test全部被执行后，才会执行一次test3。那如果我们要“或”的关系怎么办呢？首先把test3的PARENTS修改为test，记得作业是有一个复制功能的吧？再复制一份test3把它的PARENTS设成test2就好啦。</p>
<p>最后介绍一下那个<strong>Graph</strong>按钮，它可以显示一幅图来表示作业之间的依赖关系。对于我们的示例来说，这图应该是长这样的：<br><img src="/img/chronos-graph.png" alt=""></p>
<p>对于一个非常复杂的作业系统来说，这样的图能让我们很容易找到并分析作业间的依赖关系，从而采取优化措施。许多持续集成系统也都能提供类似的依赖图供管理员决策。</p>
<h2 id="u6CE8_u610F_u4E8B_u9879"><a href="#u6CE8_u610F_u4E8B_u9879" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>这个版本的chronos页面上有一些bug，会导致有时候侧面的作业详细信息栏显示有问题，刷新一下整个页面或者尝试多点击几次左边的作业表就好了。</li>
<li>还是页面的bug，有时候新建作业时会提示需要输入OWNER(S)，这是个Email地址，这样当作业出错时会通知这个Email。可是我们的测试应该是不需要错误通知的。当你看到这个提示的时候，随便输个自己的邮箱就好啦。</li>
<li>像marathon一样，chronos也支持REST API，我们可以来试一下。先随便在哪台机器找个路径生成一个<code>test4.json</code>文件，内容如下：<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">parents</span>": <span class="value">[</span><br><span class="line">    <span class="string">"test"</span></span><br><span class="line">  ]</span>,</span><br><span class="line">  "<span class="attribute">name</span>": <span class="value"><span class="string">"test4"</span></span>,</span><br><span class="line">  "<span class="attribute">command</span>": <span class="value"><span class="string">"echo test4 &gt;&gt; /tmp/test.txt"</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>然后运行以下命令来发送请求给chronos：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span>/scheduler/dependency <span class="operator">-d</span> @<span class="built_in">test</span>4.json -H <span class="string">"Content-type: application/json"</span></span><br></pre></td></tr></table></figure></p>
<p>刷新chronos的页面就能看见这个作业在跑啦。想要删掉它？当然没问题：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X DELETE http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span>/scheduler/job/<span class="built_in">test</span>4 -H <span class="string">"Content-type: application/json"</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Apache Mesos把自己定位成一个数据中心操作系统，它能管理上万台的从机（slave）。Framework相当于这个操作系统的应用程序，每当应用程序需要执行，Framework就会在Mesos中选择一台有合适资源（cpu、内存等）的从机来运行。Chronos是Framework的一种，被Airbnb公司设计用来代替cron执行作业。本文尝试从零开始用docker搭建Mesos和Chronos的运行环境，并用此环境运行作业。<br>]]>
    
    </summary>
    
      <category term="chronos" scheme="http://qinghua.github.io/tags/chronos/"/>
    
      <category term="mesos" scheme="http://qinghua.github.io/tags/mesos/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[用容器轻松搭建Marathon运行环境]]></title>
    <link href="http://qinghua.github.io/mesos-marathon/"/>
    <id>http://qinghua.github.io/mesos-marathon/</id>
    <published>2016-02-15T10:36:59.000Z</published>
    <updated>2016-02-22T07:03:48.000Z</updated>
    <content type="html"><![CDATA[<p>Apache Mesos把自己定位成一个数据中心操作系统，它能管理上万台的从机（slave）。Framework相当于这个操作系统的应用程序，每当应用程序需要执行，Framework就会在Mesos中选择一台有合适资源（cpu、内存等）的从机来运行。<a href="https://mesosphere.github.io/marathon/" target="_blank" rel="external">Marathon</a>是Framework的一种，被设计来支持长时间运行的服务。本文尝试从零开始用docker搭建Mesos和Marathon的运行环境，并用此环境长时间运行docker容器。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配两台虚拟机，一台叫做<strong>master</strong>，它的IP是<strong>192.168.33.18</strong>；另一台叫做<strong>slave</strong>，它的IP是<strong>192.168.33.19</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"master"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"master"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"slave"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"slave"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在两个终端运行以下命令启动并连接两台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh master</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh slave</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>在master的虚拟机上启动zookeeper：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=zk \</span><br><span class="line">    <span class="operator">-e</span> MYID=<span class="number">1</span> \</span><br><span class="line">    <span class="operator">-e</span> SERVERS=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    mesoscloud/zookeeper:<span class="number">3.4</span>.<span class="number">6</span>-ubuntu-<span class="number">14.04</span></span><br></pre></td></tr></table></figure></p>
<p>可以用<code>docker ps</code>看到名为zk的容器已经启动起来了。有兴趣的话，可以用下面的命令验证：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it zk zkCli.sh -server <span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">2181</span></span><br></pre></td></tr></table></figure></p>
<p>这里就不详细介绍zookeeper的命令了，<code>ls /</code>可以查看根节点，<code>help</code>可以查看所有命令，<code>quit</code>退出客户端。</p>
<p>接下来在master的虚拟机上启动mesos master：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=mm \</span><br><span class="line">    <span class="operator">-e</span> MESOS_HOSTNAME=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    <span class="operator">-e</span> MESOS_IP=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    <span class="operator">-e</span> MESOS_ZK=zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/mesos \</span><br><span class="line">    <span class="operator">-e</span> MESOS_QUORUM=<span class="number">1</span> \</span><br><span class="line">    <span class="operator">-e</span> MESOS_LOG_DIR=/var/<span class="built_in">log</span>/mesos \</span><br><span class="line">    mesoscloud/mesos-master:<span class="number">0.24</span>.<span class="number">1</span>-ubuntu-<span class="number">14.04</span></span><br></pre></td></tr></table></figure></p>
<p>顺利的话，打开<code>http://192.168.33.18:5050/</code>应该能看到下图：<br><img src="/img/mesos-master-init.png" alt=""></p>
<p>然后在slave的虚拟机上启动mesos slave：<br><figure class="highlight sh"><figcaption><span>slave</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --pid=host \</span><br><span class="line">    --privileged=<span class="literal">true</span> \</span><br><span class="line">    --name=ms1 \</span><br><span class="line">    -v /usr/bin/docker:/usr/bin/docker \</span><br><span class="line">    -v /dev:/dev \</span><br><span class="line">    -v /usr/lib/x86_64-linux-gnu/libapparmor.so.<span class="number">1</span>:/usr/lib/x86_64-linux-gnu/libapparmor.so.<span class="number">1</span>:ro \</span><br><span class="line">    -v /var/run/docker.sock:/var/run/docker.sock \</span><br><span class="line">    -v /var/<span class="built_in">log</span>/mesos:/var/<span class="built_in">log</span>/mesos \</span><br><span class="line">    -v /tmp/mesos:/tmp/mesos \</span><br><span class="line">    <span class="operator">-e</span> MESOS_HOSTNAME=<span class="number">192.168</span>.<span class="number">33.19</span> \</span><br><span class="line">    <span class="operator">-e</span> MESOS_IP=<span class="number">192.168</span>.<span class="number">33.19</span> \</span><br><span class="line">    <span class="operator">-e</span> MESOS_MASTER=zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/mesos \</span><br><span class="line">    <span class="operator">-e</span> MESOS_CONTAINERIZERS=docker,mesos \</span><br><span class="line">    mesoscloud/mesos-slave:<span class="number">0.24</span>.<span class="number">1</span>-ubuntu-<span class="number">14.04</span></span><br></pre></td></tr></table></figure></p>
<p>点击mesos页面上的<strong>Slaves</strong>应该能看到下图，这说明slave节点已经关联到mesos master了：<br><img src="/img/mesos-slaves.png" alt=""></p>
<p>最后在master的虚拟机上启动marathon：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=ma \</span><br><span class="line">    mesosphere/marathon:v0.<span class="number">15.0</span> \</span><br><span class="line">    --master zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/mesos \</span><br><span class="line">    --zk zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/marathon</span><br></pre></td></tr></table></figure></p>
<p>点击mesos页面上的<strong>Frameworks</strong>应该能看到下图，这说明marathon已经作为一个framework关联到mesos master了：<br><img src="/img/mesos-frameworks.png" alt=""></p>
<p>我们还可以打开<code>http://192.168.33.18:8080/</code>感知一下marathon：<br><img src="/img/marathon.png" alt=""></p>
<h2 id="u8FD0_u884C_u5BB9_u5668"><a href="#u8FD0_u884C_u5BB9_u5668" class="headerlink" title="运行容器"></a>运行容器</h2><p>在marathon的页面上，点击<strong>Create Application</strong>按钮，然后填入如下参数：</p>
<ul>
<li>ID：test</li>
<li>Command：sleep 30</li>
<li>Image：busybox</li>
</ul>
<p>如下图所示：<br><img src="/img/marathon-run-docker.png" alt=""></p>
<p>点击<strong>+ Create</strong>按钮，不一会儿，就能看见有一个名为<strong>test</strong>的应用程序正在运行。同时，在mesos的主页上也能看到有一个任务运行起来了。如果手慢一点或者喝了杯茶，还能看见有几个任务已经是完成的状态了，这是因为我们只让这个容器<code>sleep 30</code>存活30秒钟，如果容器自己停止了，marathon就会帮助再重新启动一个。另外还可以在slave的虚拟机上用<code>docker ps</code>看到busybox容器已经启动起来了。</p>
<h2 id="u7BA1_u7406_u5E94_u7528"><a href="#u7BA1_u7406_u5E94_u7528" class="headerlink" title="管理应用"></a>管理应用</h2><p>Marathon也能很方便地对容器进行扩缩容。当容器启动起来后，在<strong>Health</strong>栏里会有一个<strong>…</strong>的按钮，按下按钮如下图所示：<br><img src="/img/marathon-actions.png" alt=""></p>
<p>里面的<strong>Scale</strong>按钮提供了一个比较易用的方法，让我们能轻易地改变容器的数量。可以试试在弹出的界面上填2，然后点击<strong>Scale Application</strong>按钮，就会看到<strong>Running Instances</strong>很快就变成了<strong>2 of 2</strong>，点击这一行，就能看到这两个实例的状态，还可以分别下载它们的日志。同时，在mesos的主页上也能看到又多了一个运行中的任务。另外，在slave的虚拟机上用<code>docker ps</code>也能看到2个busybox的容器实例。</p>
<p>除了扩缩容，还能重新启动应用、暂停应用（实际上就是把它缩容为0个实例）和删除应用等。另外，在test这个Application的Configuration页上，还支持修改这个Application的启动参数。比如还可以在这里设置基于TCP、HTTP或者命令的健康检查，设置环境变量等多种操作。如下图所示：<br><img src="/img/marathon-edit-configuration.png" alt=""></p>
<h2 id="u6CE8_u610F_u4E8B_u9879"><a href="#u6CE8_u610F_u4E8B_u9879" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>如果第一次运行一个比较大的镜像，可能需要比较长的下载时间。在这种情况下，需要往mesos master的启动参数里增加<code>-e MESOS_EXECUTOR_REGISTRATION_TIMEOUT=10mins</code>，否则可能会报timeout的错误，默认是1分钟。还得在marathon的启动参数里增加<code>--task_launch_timeout=600000</code>，默认为300000毫秒，即5分钟。</li>
<li>如果一个容器老是启动不起来，可能是分配给它的资源太少。可以在marathon新建应用的窗口里指定分配给此应用的CPU和内存，默认是0.1和16MB。但是不要超过mesos slave能提供的资源哦。</li>
<li>如果你是坚定不移的命令行狂人，可以用<code>curl</code>来发送一个http请求来运行容器。还是用上面的<strong>test</strong>来举例子，先随便在哪台机器找个路径生成一个<code>test.json</code>文件，内容如下：<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">id</span>": <span class="value"><span class="string">"test"</span></span>,</span><br><span class="line">  "<span class="attribute">cmd</span>": <span class="value"><span class="string">"sleep 30"</span></span>,</span><br><span class="line">  "<span class="attribute">cpus</span>": <span class="value"><span class="number">0.1</span></span>,</span><br><span class="line">  "<span class="attribute">mem</span>": <span class="value"><span class="number">16.0</span></span>,</span><br><span class="line">  "<span class="attribute">container</span>": <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">type</span>": <span class="value"><span class="string">"DOCKER"</span></span>,</span><br><span class="line">    "<span class="attribute">volumes</span>": <span class="value">[]</span>,</span><br><span class="line">    "<span class="attribute">docker</span>": <span class="value">&#123;</span><br><span class="line">      "<span class="attribute">image</span>": <span class="value"><span class="string">"busybox"</span></span>,</span><br><span class="line">      "<span class="attribute">privileged</span>": <span class="value"><span class="literal">false</span></span>,</span><br><span class="line">      "<span class="attribute">parameters</span>": <span class="value">[]</span>,</span><br><span class="line">      "<span class="attribute">forcePullImage</span>": <span class="value"><span class="literal">false</span></span><br><span class="line">    </span>&#125;</span><br><span class="line">  </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>然后运行以下命令来发送请求给marathon：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span>/v2/apps <span class="operator">-d</span> @test.json -H <span class="string">"Content-type: application/json"</span></span><br></pre></td></tr></table></figure></p>
<p>打开marathon的页面就能看见这个应用在跑啦。想要删掉它？当然没问题：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X DELETE http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span>/v2/apps/<span class="built_in">test</span> -H <span class="string">"Content-type: application/json"</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Apache Mesos把自己定位成一个数据中心操作系统，它能管理上万台的从机（slave）。Framework相当于这个操作系统的应用程序，每当应用程序需要执行，Framework就会在Mesos中选择一台有合适资源（cpu、内存等）的从机来运行。<a href="https://mesosphere.github.io/marathon/">Marathon</a>是Framework的一种，被设计来支持长时间运行的服务。本文尝试从零开始用docker搭建Mesos和Marathon的运行环境，并用此环境长时间运行docker容器。<br>]]>
    
    </summary>
    
      <category term="marathon" scheme="http://qinghua.github.io/tags/marathon/"/>
    
      <category term="mesos" scheme="http://qinghua.github.io/tags/mesos/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[删除Docker Registry里的镜像怎么那么难]]></title>
    <link href="http://qinghua.github.io/docker-registry-delete/"/>
    <id>http://qinghua.github.io/docker-registry-delete/</id>
    <published>2016-02-14T10:45:20.000Z</published>
    <updated>2016-02-14T07:51:48.000Z</updated>
    <content type="html"><![CDATA[<p>除了官方的Docker Hub，Docker也提供了Docker Registry来让大家搭建自己的私有镜像库。虽然它提供了<a href="https://github.com/docker/distribution/blob/master/docs/spec/api.md#user-content-detail" target="_blank" rel="external">删除的API</a>，但是不好用。为什么小小的删除功能没弄好呢？我们该怎么办？<br><a id="more"></a></p>
<h2 id="u95EE_u9898"><a href="#u95EE_u9898" class="headerlink" title="问题"></a>问题</h2><p>有很多人抱怨说<a href="https://github.com/docker/distribution/issues/1183" target="_blank" rel="external">Docker Registry的删除功能并不会真正地释放空间</a>。虽然官方提供了API，但那些都是软删除（soft delete），只是把二进制和镜像的关系解除罢了，并不是真正的删除。真正的删除有那么困难吗？</p>
<p>目前docker官方提供了如下3个软删除的方法：</p>
<ol>
<li><code>DELETE:/v2/&lt;name&gt;/manifests/&lt;reference&gt;</code>：这个API是软删除一个<a href="https://github.com/docker/distribution/blob/master/docs/spec/manifest-v2-2.md" target="_blank" rel="external">清单（manifest）</a>，但是真正占用存储空间的层还在。</li>
<li><code>DELETE:/v2/&lt;name&gt;/blobs/&lt;digest&gt;</code>：这个API类似上面那个，只不过它要软删除的对象是层（layer）罢了。</li>
<li><code>DELETE:/v2/&lt;name&gt;/blobs/uploads/&lt;uuid&gt;</code>：这个只是取消掉另一个上传的进程罢了。</li>
</ol>
<h2 id="u96BE_u70B9"><a href="#u96BE_u70B9" class="headerlink" title="难点"></a><a href="https://github.com/docker/distribution/blob/master/ROADMAP.md#deletes" target="_blank" rel="external">难点</a></h2><p>为了删除不需要的数据，腾出磁盘空间，我们希望有删除功能。但是如果一个不健全的删除功能不小心把有用的数据给删了，那还不如没有这个功能呢。在这个逻辑前提下，docker团队选择了不删除数据。除此之外，还有一个考虑：删除功能是需要很大工作量的。大家知道程序员们的价格是比较高的，以相对便宜的磁盘空间为代价，在眼下先节省这笔人工费开销，并把它投入到更有价值的地方去，不是更有意义么。</p>
<p>那为什么删除功能需要很大的工作量呢？这是因为删除有一个大坑。首先我们来看一下存储模型：所有的数据都被存放到VFS之上，它提供了最终一致性，但是可能需要较长时间才能达到一致。再看镜像的数据结构：一个docker镜像包含了3个概念：标签（tag）、清单（manifest）和层（layer）。标签被关联到清单上，而清单则被关联到层上，就像下图一样：<br><img src="/img/docker-image.jpg" alt=""></p>
<p>其实单说删除本身其实是比较容易的事情，就像现在的垃圾收集算法一样。一个是根搜索法，从根节点开始计算，若某对象不可达，则表明不被用到，可删之。在docker镜像中并没有“根层”的概念，所以需要循环所有的清单来看是否有哪些层不被用到。还有一个是引用计数法，它为每一个对象添加一个引用计数器，为0则表明不被用到。实现起来比较简单，但是很难删除掉循环引用。在docker镜像中，因为它是一个<a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" target="_blank" rel="external">有向无环图（DAG）</a>，所以并不会有“循环引用”，正是解决这个问题的极佳方案之一。那么这个大坑在哪里呢？问题在于并发。想象一下，如果在删除某层的过程中，有另外一个push的线程误认为此层已经存在，就会在删除之后导致第二个线程push的镜像不能正常工作。</p>
<h2 id="u65B9_u6848"><a href="#u65B9_u6848" class="headerlink" title="方案"></a>方案</h2><p>目前docker官方有几个数据删除的方案（但是还没有实现）：</p>
<ol>
<li>引用计数法：如上文所述是垃圾收集算法的一种。需要维护引用计数器，对于已经存在着的docker registry来说需要数据迁移。</li>
<li>全局锁：引入GC线程来做删除。删除的时候不能写入。实现简单，但是影响性能。</li>
<li>新老代：也是引入GC线程来做删除。将存储分为年轻年老两代，GC线程删除某一代的时候允许同时写入另一代。避免了全局锁的性能问题但是实现起来比较麻烦。</li>
<li>数据库：引入一个数据库，用事务来解决并发的问题。</li>
</ol>
<p>如果你等不及docker官方的实现，并且对自己的私有库的控制力比较强，不需要考虑并发，可以使用<a href="https://github.com/burnettk/delete-docker-registry-image" target="_blank" rel="external">这个脚本</a>来彻底删除，记得先把registry停掉，或者是设置为<a href="https://github.com/docker/distribution/blob/master/docs/configuration.md#user-content-read-only-mode" target="_blank" rel="external">只读模式</a>以避免并发哦。设置一个cron任务，每天凌晨停止服务一小段时间，然后运行脚本，再启动服务就好了。</p>
<p>如果你也认为磁盘空间是比较廉价的，那么使用软删除，也就是上文介绍的官方删除API应该能够符合需求。虽然磁盘空间并没有真正地释放出来，但是删除之后镜像真的就不能再被pull下来了。记得要把<a href="https://github.com/docker/distribution/blob/master/docs/configuration.md#user-content-delete" target="_blank" rel="external">delete的设置</a>打开，否则会得到<code>The operation is unsupported</code>的异常信息。</p>
<p>如果不想使用那些感觉上奇奇怪怪的脚本，还有一个选择是设置两套docker registry，比较稳定的镜像版本放在其中一个库里，不稳定的开发版放另一个库里。每天凌晨把不稳定的版本库清空。这样的话就不会让稳定的版本库的磁盘消耗增长太快，但是也增加了一些管理的难度。没有两全其美的事啊。</p>
<h2 id="v1_vs_v2"><a href="#v1_vs_v2" class="headerlink" title="v1 vs v2"></a>v1 vs v2</h2><p>Docker Registry的老版本v1是用python写的，源码在<a href="https://github.com/docker/docker-registry" target="_blank" rel="external">这里</a>。新版本v2是用go写的，源码在<a href="https://github.com/docker/distribution" target="_blank" rel="external">这里</a>。它们的模型略有变化。老版本v1是个链表，A层链接到B层，B层链接到C层，层层组织起来一个镜像，每一层的ID都是随机生成的。这样一来浪费空间，不能实现层存储的共享，二来有安全隐患，如果不停地提交，会造成ID冲突概率提升。但也正因如此，删除的时候完全没有顾忌，真是成也萧何败也萧何啊。新版本v2的ID是对内容进行sha256哈希之后的结果，所以相同内容的层ID一定是相同的，很好地解决了v1的问题，就是删除功能需要仔细地设计才能实现。除此之外还有鉴权等其他改动，有兴趣的话可以参考<a href="http://www.csdn.net/article/2015-09-09/2825651" target="_blank" rel="external">这篇文章</a>。</p>
<p>题外话：有些人看英文容易把registry和repository搞混。在docker的领域内，repository就是相同名字镜像的集合，比如tomcat的docker repository。而registry就是提供repository服务的系统，比如Docker Hub或者是自己使用Docker Registry安装的私有库等。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>除了官方的Docker Hub，Docker也提供了Docker Registry来让大家搭建自己的私有镜像库。虽然它提供了<a href="https://github.com/docker/distribution/blob/master/docs/spec/api.md#user-content-detail">删除的API</a>，但是不好用。为什么小小的删除功能没弄好呢？我们该怎么办？<br>]]>
    
    </summary>
    
      <category term="docker" scheme="http://qinghua.github.io/tags/docker/"/>
    
      <category term="docker registry" scheme="http://qinghua.github.io/tags/docker-registry/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如果有10000台机器，你想怎么玩？（七）生命周期]]></title>
    <link href="http://qinghua.github.io/kubernetes-in-mesos-7/"/>
    <id>http://qinghua.github.io/kubernetes-in-mesos-7/</id>
    <published>2016-02-03T10:41:27.000Z</published>
    <updated>2016-02-03T13:27:40.000Z</updated>
    <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的pod生命周期管理。涉及到k8s提供的一系列小功能。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a><a id="more"></a>
</li>
</ul>
<h2 id="u5065_u5EB7_u68C0_u67E5"><a href="#u5065_u5EB7_u68C0_u67E5" class="headerlink" title="健康检查"></a>健康检查</h2><p>一个pod在运行中，难免出现容器还好好地跑着，但是却不正常工作的情况。Kubernetes的做法是引入定时<a href="http://kubernetes.io/v1.1/docs/user-guide/pod-states.html#container-probes" target="_blank" rel="external">健康检查</a>，如果健康检查失败，就把这个容器杀掉，然后kubelet就会重新启动一个容器来代替它。目前支持两种健康检查的机制：</p>
<ul>
<li><code>LivenessProbe</code>：如果健康检查失败，就把这个容器杀掉，然后kubelet根据预先设置的<a href="http://kubernetes.io/v1.1/docs/user-guide/pod-states.html#restartpolicy" target="_blank" rel="external">重启规则</a>来决定怎么处理：啥也不干、挂了才重启或者总是重启。</li>
<li><code>ReadinessProbe</code>：如果健康检查失败，这个pod的IP地址将会从endpoints里移除，所以相当于屏蔽这个pod提供的服务而不是将它杀掉。</li>
</ul>
<p>那健康检查怎么做呢？可以是一段脚本，返回非0就代表错误；可以说一个http请求，返回200~400之间代表成功；还可以是一个tcp端口，打开即算成功。可以通过设置kubelet的参数<code>--sync-frequency</code>来设置健康检查的间隔时间。还可以在设置probe的时候指定健康检查的超时时间和第一次健康检查的延时（从容器启动完毕开始）。</p>
<h2 id="u94A9_u5B50_uFF08hook_uFF09"><a href="#u94A9_u5B50_uFF08hook_uFF09" class="headerlink" title="钩子（hook）"></a>钩子（hook）</h2><p>有时候我们需要在pod启动完成或者快要关闭的时候做点儿事情。做的事情可以是执行脚本或者发出http请求，越轻量级越好。Kubernetes提供了两个<a href="http://kubernetes.io/v1.1/docs/user-guide/container-environment.html#container-hooks" target="_blank" rel="external">钩子</a>来做这样的事：</p>
<ul>
<li>postStart：当一个容器被创建成功的时候</li>
<li>preStop：当一个容器即将被关闭的时候</li>
</ul>
<p>钩子的设计理念是“宁滥毋缺”，所以某些情况下它是有可能被执行多次的，设计自己的钩子时需要考虑这样的情况，尽量使操作“无状态”。</p>
<h2 id="u4E00_u6B21_u6027_u4EFB_u52A1"><a href="#u4E00_u6B21_u6027_u4EFB_u52A1" class="headerlink" title="一次性任务"></a>一次性任务</h2><p>Kubernetes除了支持服务，也支持一次性任务<a href="https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/jobs.md" target="_blank" rel="external">Job</a>。这个概念在Kubernetes 1.1版中已经有了，<a href="https://github.com/kubernetes/kubernetes/wiki/Release-1.2" target="_blank" rel="external">1.2版</a>才算开发完成。因为一次性任务结束以后还重启没有意义，所以它不支持“总是重启”的重启规则。Job有三种类型：</p>
<ul>
<li>非并行：就是启动一个pod，当pod成功结束了就算是job完成了</li>
<li>固定数量并行：并行启动固定数量个pod，每个pod都成功结束了就算是job完成了</li>
<li>工作队列并行：行启动多个pod，其中一个pod成功结束了，其他pod就开始停止运行。当全部pod都停止了就算是job完成了</li>
</ul>
<p>其实我们当然也能不使用job这个概念而直接启动一个pod来完成我们的一次性任务。可是如果pod运行过程中那个node要是挂掉了那就糟糕了。对了，rc不就是来保证pod总是有实例在运行的机制吗？那么为啥我们还需要job这个概念呢？原来rc是为永不停止的pod设计的，而job是为需要停止的pod设计的，就这么简单。</p>
<h2 id="u6269_u5BB9_u7F29_u5BB9"><a href="#u6269_u5BB9_u7F29_u5BB9" class="headerlink" title="扩容缩容"></a>扩容缩容</h2><p>虚拟机级别上，Mesos也能轻松做到动态增删slave，从而为kubernetes提供更多的offer；与此同时，kubernetes也支持动态增删节点。容器级别上，Kubernetes的replication controller可以很容易地对pod进行扩缩容。值得一提的是，Kubernetes在删除pod的时候并不会把容器删除，是出于可能需要在以后查看日志的<a href="https://github.com/kubernetes/kubernetes/issues/1148" target="_blank" rel="external">考虑</a>。</p>
<h2 id="u6EDA_u52A8_u5347_u7EA7"><a href="#u6EDA_u52A8_u5347_u7EA7" class="headerlink" title="滚动升级"></a>滚动升级</h2><p>Kubernetes的replication controller还支持滚动升级（<a href="http://kubernetes.io/v1.1/docs/user-guide/replication-controller.html#rolling-updates" target="_blank" rel="external">rolling update</a>）。当我们想用新版本的镜像来代替已经部署的旧容器的时候，这个特性能用类似蓝绿部署的方式帮我们轻易升级。这个方式是：创建一个副本数为1的rc并关联到新pod，逐渐增加它的副本数并减少旧rc的副本数，最终完全替代。讲起来挺生涩，其实很简单：举个栗子，有一个既存服务来自于3个pod，我们希望用新的pod来代替旧的。这是现在的情况：<br><img src="/img/rolling-update-1.png" alt=""></p>
<p>Kubernetes启动了一个新的rc，它有一个新的pod，并关联到服务上去。<br><img src="/img/rolling-update-2.png" alt=""></p>
<p>然后停掉一个旧的pod，保持这个服务的pod总数还是3个：<br><img src="/img/rolling-update-3.png" alt=""></p>
<p>继续增加新pod：<br><img src="/img/rolling-update-4.png" alt=""></p>
<p>继续停掉旧pod：<br><img src="/img/rolling-update-5.png" alt=""></p>
<p>增加新pod：<br><img src="/img/rolling-update-6.png" alt=""></p>
<p>停掉旧pod，再把没用了的旧rc也删掉：<br><img src="/img/rolling-update-7.png" alt=""></p>
<p>这样新服务就起来了！中间的替换速度是可以由我们设定的，还支持回滚。在<a href="http://kubernetes.io/v1.1/docs/user-guide/update-demo/README.html" target="_blank" rel="external">这里</a>有一个例子可供参考。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这次聊聊mesos+k8s的pod生命周期管理。涉及到k8s提供的一系列小功能。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a>]]>
    
    </summary>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="mesos" scheme="http://qinghua.github.io/tags/mesos/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如果有10000台机器，你想怎么玩？（六）性能]]></title>
    <link href="http://qinghua.github.io/kubernetes-in-mesos-6/"/>
    <id>http://qinghua.github.io/kubernetes-in-mesos-6/</id>
    <published>2016-02-02T11:52:32.000Z</published>
    <updated>2016-02-14T11:02:14.000Z</updated>
    <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的性能。纯粹的kubernetes v1.1可以支持250个节点，但是一跟mesos结合起来，由于需要等待、接受资源邀约等行为，确实会更慢一些。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a><a id="more"></a>
</li>
</ul>
<h2 id="u5173_u4E8E_u6027_u80FD"><a href="#u5173_u4E8E_u6027_u80FD" class="headerlink" title="关于性能"></a>关于性能</h2><p>有一篇很知名的<a href="http://blog.kubernetes.io/2015/09/kubernetes-performance-measurements-and.html" target="_blank" rel="external">kubernetes性能测试文章</a>，提到了不少性能测试的考量、结果和计划，也有<a href="http://dockone.io/article/677" target="_blank" rel="external">中文译文</a>。相信看完后对kubernetes自身的性能会有一些感性认识。</p>
<p>Kubernetes v1.0仅仅支持100个节点，kubernetes v1.1已经可以支持250个节点了。官方也希望能<a href="https://github.com/kubernetes/kubernetes/blob/master/docs/admin/multi-cluster.md#user-content-selecting-the-right-number-of-clusters" target="_blank" rel="external">在2016年初支持1000个节点</a>。Kubernetes还提供了一个性能测试工具<a href="https://github.com/kubernetes/kubernetes/blob/master/docs/devel/kubemark-guide.md" target="_blank" rel="external">Kubemark</a>。它由真实的master和虚拟的空节点组成，默认跑在<a href="https://cloud.google.com/container-engine/" target="_blank" rel="external">GCE</a>上，这样不需要大量机器便可以进行性能测试了。</p>
<h2 id="u6CE8_u610F_u4E8B_u9879"><a href="#u6CE8_u610F_u4E8B_u9879" class="headerlink" title="注意事项"></a>注意事项</h2><p>根据<a href="https://github.com/kubernetes/kubernetes/blob/master/docs/admin/cluster-large.md" target="_blank" rel="external">kubernetes大集群</a>这篇文章的描述，1.1版本支持最大250个节点，每个节点30个pod，每个pod 1~2个容器。当使用50台以上的节点时，最好用单独的etcd来存储事件。可以在kubernetes的api server启动参数里配置类似<code>--etcd-servers-overrides=/events#http://192.168.33.11:4001</code>这样的值来分离事件etcd。</p>
<p>如果想尽可能的模拟生产环境，所以在测试环境中使用kubernetes自身的系统插件（如DNS、Heapster、ElasticSearch等）时，也需要注意由于集群规模的增大，默认的插件资源有可能不够，从而导致OOM最终使插件不停地挂掉重启。可以通过配置resources的<a href="http://kubernetes.io/v1.1/docs/design/resources.html#resource-specifications" target="_blank" rel="external">limit</a>来增大插件的内存供给。</p>
<p>Docker的性能方面，由于Ubuntu的docker存储驱动默认使用AUFS，速度比Device Mapper快上不少。所以如果用CentOS来做node，会明显感觉容器的启动删除都比较慢。网上也有<a href="https://www.linux-toys.com/?p=374" target="_blank" rel="external">文章</a>指出这点，笔者测试的感觉与之相符。</p>
<h2 id="u6027_u80FD_u8C03_u4F18"><a href="#u6027_u80FD_u8C03_u4F18" class="headerlink" title="性能调优"></a>性能调优</h2><h3 id="Mesos+Kubernetes"><a href="#Mesos+Kubernetes" class="headerlink" title="Mesos+Kubernetes"></a>Mesos+Kubernetes</h3><p>Mesos的资源分配现在是酱紫的：</p>
<ol>
<li>slave告诉master自己有什么资源</li>
<li>master把这个资源包装成offer发送给framework（这里是kubernetes）</li>
<li>framework接受或拒绝</li>
<li>若是framework接受了，让slave运行任务<br><img src="/img/mesos-architecture.jpg" alt=""></li>
</ol>
<p>有一点需要注意的是mesos master并不是一收到slave的资源便把它发送给framework的。想象一下如果有1000台机器的话，那offer的发送频率得是什么样子。Mesos master是每隔一段时间发送一次。它的启动参数里有一个<code>--allocation_interval</code>，它决定了这个间隔时间，默认为1秒。当有多个slave的时候，有可能master在前一秒告诉framework有10份offer，后一秒又告诉说现在有15份（有些被拒绝的offer回来了，有些新slave能提供新offer，有些被接受了，但是还有余裕…）。</p>
<p>Mesos+Kubernetes的<a href="https://github.com/kubernetes/kubernetes/blob/master/contrib/mesos/docs/issues.md#scheduling" target="_blank" rel="external">分配算法</a>现在还很原始：一个offer只会运行一个pod。所以如果请求启动的大量的pod的时候，就需要很多个offer来运行这些pod。为了提高性能，一个办法是在一个offer里安排多个pods，这个是kubernetes的未来计划，现在还不是我们的菜。另一个办法是提高mesos master的offer频率。虽然把<code>--allocation_interval</code>调低可以增加offer发送频率，但是如果offer回流得很慢，那又有什么意义呢。所以kubernetes的scheduler处理得越久，offer的流动性就越差，pod的启动速度就越慢。接着往下走，如何提高scheduler的处理速度呢？最简单的处理办法：换高配！4C8G的虚拟机撑死能扛住250台mesos slave。软件上就还得靠优化scheduler的流程了。另外，mesos master由于要不断发offer出去，还要处理被接受或拒绝的offer，也需要比较强的配置，但是kubernetes master的配置影响力更大，需要相对更好的配置。</p>
<p>Mesos+Kubernetes的scheduler还支持一些细粒度的性能调优，有兴趣的朋友可以去<a href="https://github.com/kubernetes/kubernetes/blob/master/contrib/mesos/docs/scheduler.md#user-content-tuning" target="_blank" rel="external">看一看</a>。</p>
<h3 id="u7EAFkubernetes"><a href="#u7EAFkubernetes" class="headerlink" title="纯kubernetes"></a>纯kubernetes</h3><p>不带mesos玩儿的kubernetes会简单一些，它的scheduler支持一个参数<code>--bind-pods-qps</code>，这个值决定每秒启动的pod数，默认为50。可以根据机器和网络性能相对应地调节。</p>
<h2 id="u6D4B_u8BD5_u7ED3_u679C"><a href="#u6D4B_u8BD5_u7ED3_u679C" class="headerlink" title="测试结果"></a>测试结果</h2><h3 id="Mesos+Kubernetes-1"><a href="#Mesos+Kubernetes-1" class="headerlink" title="Mesos+Kubernetes"></a>Mesos+Kubernetes</h3><p>Mesos+Kubernetes的情况下，100台mesos slave的情况下，启动100个pod需要将近50秒。由上可知，由于offer是比较均匀的，pod的创建时间基本上也是均匀的。这就意味着启动500个pod需要将近250秒。而且，pod还是比较平均地分布在所有slave上的。删除pod的话，因为无关offer，所以就不是线性关系了。100个pod需要10~15秒，如果一口气删得多一些，需要的时间会比线性增加的时间少一些。250台mesos slave的情况下，基本上kubernetes就带不动了，api server的cpu占用率很高。</p>
<h3 id="u7EAFkubernetes-1"><a href="#u7EAFkubernetes-1" class="headerlink" title="纯kubernetes"></a>纯kubernetes</h3><p>不带mesos玩儿的kubernetes在100台节点的情况下，速度要快得多：启动100个pod仅需10秒左右，1000个约80秒。如果分配超过3000的pod，就会出现部分pod起不来的情况。笔者试验了4000个pod，有280个起不来。250台节点的时候也没有什么压力，性能上比Mesos+Kubernetes好了不止一星半点。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这次聊聊mesos+k8s的性能。纯粹的kubernetes v1.1可以支持250个节点，但是一跟mesos结合起来，由于需要等待、接受资源邀约等行为，确实会更慢一些。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a>]]>
    
    </summary>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="mesos" scheme="http://qinghua.github.io/tags/mesos/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Docker的存储是怎么工作的]]></title>
    <link href="http://qinghua.github.io/docker-storage/"/>
    <id>http://qinghua.github.io/docker-storage/</id>
    <published>2016-01-29T10:00:00.000Z</published>
    <updated>2016-01-30T12:14:19.000Z</updated>
    <content type="html"><![CDATA[<p>我们都知道docker支持多种存储驱动，默认在ubuntu上使用AUFS，其他Linux系统上使用devicemapper。这篇文章从零开始，用一些Linux的命令来使用这些不同的存储，包括AUFS、Device Mapper、Btrfs和Overlay。<br><a id="more"></a></p>
<h2 id="u80CC_u666F_u77E5_u8BC6"><a href="#u80CC_u666F_u77E5_u8BC6" class="headerlink" title="背景知识"></a>背景知识</h2><p>Docker最早只是运行在Ubuntu和Debian上，使用的存储驱动是AUFS。随着Docker越来越流行，很多人都希望能把它运行在RHEL系列上。可是Linux内核和RHEL并不支持AUFS，最后红帽公司和Docker公司一起合作开发了基于Device Mapper技术的devicemapper存储驱动，这也成为Docker支持的第二款存储。由于Linux内核2.6.9就已经包含Device Mapper技术了，所以它也非常的稳定，代价是比较慢。<a href="https://en.wikipedia.org/wiki/ZFS" target="_blank" rel="external">ZFS</a>是被Oracle收购的Sun公司为Solaris 10开发的新一代文件系统，支持快照，克隆，<a href="https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/#the-copy-on-write-strategy" target="_blank" rel="external">写时复制</a>（CoW）等。ZFS的“Z”是最后一个字母，表示终极文件系统，不需要开发其它的文件系统了。虽然ZFS各种好，但是毕竟它的Linux版本是移植过来的，Docker官方并不推荐在生产环境上使用，除非你对ZFS相当熟悉。而且由于软件许可证不同的关系，它也无法被合并进Linux内核里。这么NB的文件系统出来后，Linux社区也有所回应。<a href="https://btrfs.wiki.kernel.org/index.php/Main_Page" target="_blank" rel="external">Btrfs</a>就是和ZFS比较类似的Linux原生存储系统，在Linux内核2.6.29里就包含它了。虽然Btrfs未来是要替换devicemapper的，但是目前devicemapper更安全，更稳定，更适合生产环境。所以如果不是有很经验的话，也不那么推荐在生产环境使用。<a href="https://en.wikipedia.org/wiki/OverlayFS" target="_blank" rel="external">OverlayFS</a>是类似AUFS的<a href="https://en.wikipedia.org/wiki/UnionFS" target="_blank" rel="external">联合文件系统</a>，但是轻量级一些，而且还能快一点儿。更重要的是，它已经被合并到Linux内核3.18版了。虽然OverlayFS发展得很快，但是它还非常年轻，如果要上生产系统，还是要记得小心为上。Docker还支持一个<a href="https://en.wikipedia.org/wiki/Virtual_file_system" target="_blank" rel="external">VFS</a>驱动，它是一个中间层的抽象，底层支持ext系列，ntfs，nfs等等，对上层提供一个标准的文件操作接口，很早就被包含到Linux内核里了。但是由于它不支持写时复制，所以比较占磁盘空间，速度也慢，同样并不推荐上生产环境。</p>
<p>说到这里，好几个存储驱动都上Linux内核了，怎么AUFS一直被拒于门外呢？AUFS是一个日本人岡島順治郎开发的，他也曾希望能把这个存储驱动提交到内核中。但是据说<a href="http://www.programering.com/a/MTM0YDNwATQ.html" target="_blank" rel="external">Linus Torvalds有点儿嫌弃AUFS的代码写得烂</a>……</p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令启动并连接虚拟机：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="AUFS"><a href="#AUFS" class="headerlink" title="AUFS"></a>AUFS</h2><p>AUFS是一个联合文件系统，也就是说，它是一层层垒上去的文件系统。最上层能看到的就是下层的所有系统合并后的结果。我们创建几个文件夹，layer1是最底层，result用来挂载，再搞几个文件：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/aufs</span><br><span class="line"><span class="built_in">cd</span> ~/aufs</span><br><span class="line"></span><br><span class="line">mkdir layer1 layer2 result</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"file1 in layer1"</span> &gt; layer1/file1</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"file2 in layer1"</span> &gt; layer1/file2</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"file1 in layer2"</span> &gt; layer2/file1</span><br></pre></td></tr></table></figure></p>
<p>现在文件夹的层级结构看起来是酱紫的：</p>
<p><pre><br>└── aufs<br>    ├── layer1<br>    │   ├── file1    # file1 in layer1<br>    │   └── file2    # file2 in layer1<br>    ├── layer2<br>    │   └── file1    # file1 in layer2<br>    └── result<br></pre><br>然后一层层地挂载到result文件夹去（none的意思是挂载的不是设备文件），就能看到result现在有两个文件，以及它们的内容：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t aufs -o br=layer2=rw:layer1=ro none result</span><br><span class="line"></span><br><span class="line">ls result</span><br><span class="line">cat result/file1    <span class="comment"># file1 in layer2</span></span><br><span class="line">cat result/file2    <span class="comment"># file2 in layer1</span></span><br></pre></td></tr></table></figure></p>
<p>file1是由layer2提供的，file2是由layer1提供的，因为layer2里没有file2。如果我们在挂载后的目录写入file1~3：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"file1 in result"</span> &gt; result/file1</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"file2 in result"</span> &gt; result/file2</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"file3 in result"</span> &gt; result/file3</span><br><span class="line"></span><br><span class="line">cat layer1/file1    <span class="comment"># file1 in layer1</span></span><br><span class="line">cat layer1/file2    <span class="comment"># file2 in layer1</span></span><br><span class="line">cat layer2/file1    <span class="comment"># file1 in result</span></span><br><span class="line">cat layer2/file2    <span class="comment"># file2 in result</span></span><br><span class="line">cat layer2/file3    <span class="comment"># file3 in result</span></span><br></pre></td></tr></table></figure></p>
<p>就会看到这些文件都是写入到layer2的。测试完成，清场~<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo umount result</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">rm -rf aufs</span><br></pre></td></tr></table></figure></p>
<p>想要了解更细致点的话可以参考<a href="http://coolshell.cn/articles/17061.html" target="_blank" rel="external">Docker基础技术：AUFS</a>这篇文章。</p>
<p>回头来看Docker官方的这幅图：<br><img src="https://docs.docker.com/engine/userguide/storagedriver/images/aufs_delete.jpg" alt=""></p>
<p>虽然是删除文件的示例，但是也能清楚看到AUFS是怎么工作的。然后再结合docker一起看：<br><img src="https://docs.docker.com/engine/userguide/storagedriver/images/aufs_layers.jpg" alt=""></p>
<p>一切就都很清楚明了：一层层地累加所有的文件，最终加载到镜像里。</p>
<h2 id="Device_Mapper"><a href="#Device_Mapper" class="headerlink" title="Device Mapper"></a>Device Mapper</h2><p>Device Mapper是块设备的驱动，它的写时复制是基于块而非文件的。它包含3个概念：原设备，快照和映射表，它们的关系是：原设备通过映射表映射到快照去。一个快照只能有一个原设备，而一个原设备可以映射成多个快照。快照还能作为原设备映射到其他快照中，理论上可以无限迭代。</p>
<p>Device Mapper还提供了一种<a href="https://www.kernel.org/doc/Documentation/device-mapper/thin-provisioning.txt" target="_blank" rel="external">Thin-Provisioning</a>技术。它实际上就是允许存储的超卖，用以提升空间利用率。当它和快照结合起来的时候，就可以做到许多快照挂载在一个原设备上，除非某个快照发生写操作，不然不会真正给快照们分配空间。这样的原设备叫做<a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Logical_Volume_Manager_Administration/thinprovisioned_volumes.html" target="_blank" rel="external">Thin Volume</a>，它和快照都会由thin-pool来分配，超卖就发生在thin-pool之上。它需要两个设备用来存放实际数据和元数据。下面我们来创建两个文件，用来充当实际数据文件和元数据文件：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/devicemapper</span><br><span class="line"><span class="built_in">cd</span> ~/devicemapper</span><br><span class="line"></span><br><span class="line">mkdir thin</span><br><span class="line">mkdir snap1</span><br><span class="line">mkdir snap11</span><br><span class="line">mkdir snap12</span><br><span class="line"></span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=metadata.img bs=<span class="number">1024</span>K count=<span class="number">1</span></span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=data.img bs=<span class="number">1024</span>K count=<span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<p>文件建好了之后，用<a href="https://en.wikipedia.org/wiki/Loop_device" target="_blank" rel="external">Loop device</a>把它们模拟成块设备：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo losetup /dev/loop0 metadata.img</span><br><span class="line">sudo losetup /dev/loop1 data.img</span><br><span class="line">sudo losetup <span class="operator">-a</span></span><br></pre></td></tr></table></figure></p>
<p>然后创建thin-pool（参数的含义可以参考<a href="http://coolshell.cn/articles/17200.html" target="_blank" rel="external">这篇文章</a>）：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup create pool --table <span class="string">"0 20480 thin-pool /dev/loop0 /dev/loop1 128 32768 1 skip_block_zeroing"</span></span><br><span class="line">ls /dev/mapper/    <span class="comment"># 这里就会多一个pool</span></span><br></pre></td></tr></table></figure></p>
<p>之后创建Thin Volume并格式化：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup message /dev/mapper/pool <span class="number">0</span> <span class="string">"create_thin 0"</span></span><br><span class="line">sudo dmsetup create thin --table <span class="string">"0 2048 thin /dev/mapper/pool 0"</span></span><br><span class="line">sudo mkfs.ext4 /dev/mapper/thin</span><br><span class="line">ls /dev/mapper/    <span class="comment"># 这里又会多一个thin</span></span><br></pre></td></tr></table></figure></p>
<p>加载这个Thin Volume并往里写个文件。我的测试机器上需80秒左右才能把这个文件同步回thin-pool去。如果不等待，可能接下来的快照里就不会有这个文件；如果等待时间不足（小于30秒），可能快照里会有这个文件，但是内容为空。这个时间跟thin-pool的参数，尤其是先前创建的实际数据和元数据文件有关。<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mount /dev/mapper/thin thin</span><br><span class="line">sudo sh -c <span class="string">"echo file1 in thin &gt; thin/file1"</span></span><br><span class="line">sleep <span class="number">80</span>s</span><br></pre></td></tr></table></figure></p>
<p>睡饱后，给thin这个原设备添加一份快照snap1：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup message /dev/mapper/pool <span class="number">0</span> <span class="string">"create_snap 1 0"</span></span><br><span class="line">sudo dmsetup create snap1 --table <span class="string">"0 2048 thin /dev/mapper/pool 1"</span></span><br><span class="line">ls /dev/mapper/    <span class="comment"># 这里又会多一个snap1</span></span><br></pre></td></tr></table></figure></p>
<p>加载这个快照，能看见先前写的file1文件被同步过来了。再往里写个新文件。还是要保证睡眠充足：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo mount /dev/mapper/snap1 snap1</span><br><span class="line">sudo ls <span class="operator">-l</span> snap1</span><br><span class="line">sudo cat snap1/file1    <span class="comment"># file1 in thin</span></span><br><span class="line">sudo sh -c <span class="string">"echo file2 in snap1 &gt; snap1/file2"</span></span><br><span class="line">sleep <span class="number">80</span>s</span><br></pre></td></tr></table></figure></p>
<p>快照是能作为原设备映射成其他快照的，下面从snap1映射一份snap11：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup message /dev/mapper/pool <span class="number">0</span> <span class="string">"create_snap 2 1"</span></span><br><span class="line">sudo dmsetup create snap11 --table <span class="string">"0 2048 thin /dev/mapper/pool 2"</span></span><br></pre></td></tr></table></figure></p>
<p>加载完后就能看到file1和file2都被同步过来了：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo mount /dev/mapper/snap11 snap11</span><br><span class="line">sudo ls <span class="operator">-l</span> snap11</span><br><span class="line">sudo cat snap11/file1    <span class="comment"># file1 in thin</span></span><br><span class="line">sudo cat snap11/file2    <span class="comment"># file2 in snap1</span></span><br></pre></td></tr></table></figure></p>
<p>一份原设备是可以映射成多个快照的，下面从snap1再映射一份snap12：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup message /dev/mapper/pool <span class="number">0</span> <span class="string">"create_snap 3 1"</span></span><br><span class="line">sudo dmsetup create snap12 --table <span class="string">"0 2048 thin /dev/mapper/pool 3"</span></span><br><span class="line"></span><br><span class="line">sudo mount /dev/mapper/snap12 snap12</span><br><span class="line">sudo ls <span class="operator">-l</span> snap12</span><br><span class="line">sudo cat snap11/file1    <span class="comment"># file1 in thin</span></span><br><span class="line">sudo cat snap11/file2    <span class="comment"># file2 in snap1</span></span><br></pre></td></tr></table></figure></p>
<p>测试完成，清场~<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo umount snap1</span><br><span class="line">sudo umount snap11</span><br><span class="line">sudo umount snap12</span><br><span class="line">sudo umount thin</span><br><span class="line"></span><br><span class="line">sudo dmsetup remove snap11</span><br><span class="line">sudo dmsetup remove snap12</span><br><span class="line">sudo dmsetup remove snap1</span><br><span class="line">sudo dmsetup remove thin</span><br><span class="line">sudo dmsetup remove pool</span><br><span class="line"></span><br><span class="line">sudo losetup <span class="operator">-d</span> /dev/loop0</span><br><span class="line">sudo losetup <span class="operator">-d</span> /dev/loop1</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">rm -rf devicemapper</span><br></pre></td></tr></table></figure></p>
<p>在RHEL，CentOS系列上，Docker默认使用loop-lvm，类似上文的机制（配的是<a href="https://en.wikipedia.org/wiki/Sparse_file" target="_blank" rel="external">稀疏文件</a>），虽然性能比本文要好，但也是堪忧。官方推荐使用<a href="https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/#other-device-mapper-performance-considerations" target="_blank" rel="external">direct-lvm</a>，也就是直接使用raw分区。<a href="http://blog.opskumu.com/docker-storage-setup.html" target="_blank" rel="external">这篇文章</a>介绍了如何在CentOS 7上使用direct-lvm。另外，<a href="http://www.infoq.com/cn/articles/analysis-of-docker-file-system-aufs-and-devicemapper/" target="_blank" rel="external">剖析Docker文件系统</a>对AUFS和Device Mapper有很详细的讲解。</p>
<p>回头来看Docker官方的这幅图：<br><img src="http://farm1.staticflickr.com/703/22116692899_0471e5e160_b.jpg" alt=""></p>
<p>一切就都很清楚明了：最底层是两个文件：数据和元数据文件。这两个文件上面是一个pool，再上面是一个原设备，然后就是一层层的快照叠加上去，直至镜像，充分共享了存储空间。</p>
<h2 id="Btrfs"><a href="#Btrfs" class="headerlink" title="Btrfs"></a>Btrfs</h2><p>Btrfs的Btr是B-tree的意思，元数据用B树管理，比较高效。它也支持块级别的写时复制，性能也不错，对SSD有优化，但是不支持SELinux。它支持把文件系统的一部分配置为<a href="https://btrfs.wiki.kernel.org/index.php/SysadminGuide#Subvolumes" target="_blank" rel="external">Subvolume</a>子文件系统，父文件系统就像一个pool一样给这些子文件系统们提供底层的存储空间。这就意味着子文件系统无需关心设置各自的大小，反正背后有父文件系统撑腰。Btrfs还支持对子文件系统的快照，速度非常快，起码比Device Mapper快多了。快照在Btrfs里也是一等公民，同样也可以像Subvolume那样再快照、被加载，享受写时复制技术。</p>
<p>要使用Btrfs，得先安装工具包：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install btrfs-tools</span><br></pre></td></tr></table></figure></p>
<p>下面我们来创建一个文件，用Loop device把它模拟成块设备：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/btrfs</span><br><span class="line"><span class="built_in">cd</span> ~/btrfs</span><br><span class="line"></span><br><span class="line">mkdir result</span><br><span class="line"></span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=data.img bs=<span class="number">1024</span>K count=<span class="number">10</span></span><br><span class="line"></span><br><span class="line">sudo losetup /dev/loop0 data.img</span><br><span class="line">sudo losetup <span class="operator">-a</span></span><br></pre></td></tr></table></figure></p>
<p>把这个块设备格式化成btrfs：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkfs.btrfs <span class="operator">-f</span> /dev/loop0</span><br><span class="line">sudo mount /dev/loop0 result/</span><br></pre></td></tr></table></figure></p>
<p>新建一个subvolumn并往里写个文件：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs subvolume create result/origin/</span><br><span class="line">sudo sh -c <span class="string">"echo file1 in origin &gt; result/origin/file1"</span></span><br></pre></td></tr></table></figure></p>
<p>给result/origin这个subvolumn添加一份快照snap1，能看见先前写的file1文件被同步过来了。再往里写个新文件：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs subvolume snapshot result/origin/ result/snap1</span><br><span class="line">sudo ls <span class="operator">-l</span> result/snap1</span><br><span class="line">sudo cat result/snap1/file1    <span class="comment"># file1 in origin</span></span><br><span class="line">sudo sh -c <span class="string">"echo file2 in snap1 &gt; result/snap1/file2"</span></span><br></pre></td></tr></table></figure></p>
<p>快照也像Device Mapper那样能生成其他的快照，下面从snap1生成一份snap11：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs subvolume snapshot result/snap1/ result/snap11</span><br><span class="line">sudo ls <span class="operator">-l</span> result/snap11</span><br><span class="line">sudo cat result/snap11/file1    <span class="comment"># file1 in origin</span></span><br><span class="line">sudo cat result/snap11/file2    <span class="comment"># file2 in snap1</span></span><br></pre></td></tr></table></figure></p>
<p>也是可以生成多个快照的，下面从snap1再生成一份snap12：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs subvolume snapshot result/snap1/ result/snap12</span><br><span class="line">sudo ls <span class="operator">-l</span> result/snap12</span><br><span class="line">sudo cat result/snap12/file1    <span class="comment"># file1 in origin</span></span><br><span class="line">sudo cat result/snap12/file2    <span class="comment"># file2 in snap1</span></span><br></pre></td></tr></table></figure></p>
<p>可以使用这个命令来查看所有快照：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs subvolume list result</span><br></pre></td></tr></table></figure></p>
<p>可以使用这个命令来查看这个文件系统：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs filesystem show /dev/loop0</span><br></pre></td></tr></table></figure></p>
<p>测试完成，清场~<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo umount result</span><br><span class="line"></span><br><span class="line">sudo losetup <span class="operator">-d</span> /dev/loop0</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">rm -rf btrfs</span><br></pre></td></tr></table></figure></p>
<p>我们看到它比Device Mapper更简单一些，并且速度很快，不需要sleep以待同步完成。<a href="http://www.ibm.com/developerworks/cn/linux/l-cn-btrfs/index.html" target="_blank" rel="external">这篇文章</a>虽然有点儿旧了，但是对Btrfs的原理讲得挺清楚的。</p>
<p>回头来看Docker官方的这幅图：<br><img src="https://docs.docker.com/engine/userguide/storagedriver/images/btfs_container_layer.jpg" alt=""></p>
<p>一切就都很清楚明了：最底层是个subvolume，再它之上层层累加快照，镜像也不例外。</p>
<h2 id="Overlay"><a href="#Overlay" class="headerlink" title="Overlay"></a>Overlay</h2><p>最初它叫做OverlayFS，后来被合并进Linux内核的时候被改名为Overlay。它和AUFS一样都是联合文件系统。Overlay由两层文件系统组成：upper（上层）和lower（下层）。下层可以是只读的任意的Linux支持的文件系统，甚至可以是另一个Overlay，而上层一般是可读写的。所以模型上比AUFS要简单一些，这就是为什么我们会认为它更轻量级一些。</p>
<p>用<code>uname -r</code>可以看到我们现在这个vagrant虚拟机的Linux内核版本是3.13，而内核3.18之后才支持Overlay，所以我们得先升级一下内核，否则在mount的时候会出错：<code>mount: wrong fs type, bad option, bad superblock on overlay</code>。运行以下命令来升级ubuntu 14.04的内核：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /tmp/</span><br><span class="line"></span><br><span class="line">wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v3.<span class="number">18</span>-vivid/linux-headers-<span class="number">3.18</span>.<span class="number">0</span>-<span class="number">031800</span>-generic_3.<span class="number">18.0</span>-<span class="number">031800.201412071935</span>_amd64.deb</span><br><span class="line">wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v3.<span class="number">18</span>-vivid/linux-headers-<span class="number">3.18</span>.<span class="number">0</span>-<span class="number">031800</span>_3.<span class="number">18.0</span>-<span class="number">031800.201412071935</span>_all.deb</span><br><span class="line">wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v3.<span class="number">18</span>-vivid/linux-image-<span class="number">3.18</span>.<span class="number">0</span>-<span class="number">031800</span>-generic_3.<span class="number">18.0</span>-<span class="number">031800.201412071935</span>_amd64.deb</span><br><span class="line">sudo dpkg -i linux-headers-<span class="number">3.18</span>.<span class="number">0</span>-*.deb linux-image-<span class="number">3.18</span>.<span class="number">0</span>-*.deb</span><br><span class="line">sudo update-grub</span><br><span class="line"></span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure></p>
<p>等待重启之后，重新连接进vagrant虚拟机：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<p>完成之后再用<code>uname -r</code>看一下，现在应该已经是3.18了。下面我们开搞吧：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/overlay</span><br><span class="line"><span class="built_in">cd</span> ~/overlay</span><br><span class="line"></span><br><span class="line">mkdir lower upper work merged</span><br><span class="line"><span class="built_in">echo</span> file1 <span class="keyword">in</span> lower &gt; lower/file1</span><br><span class="line"><span class="built_in">echo</span> file2 <span class="keyword">in</span> lower &gt; lower/file2</span><br><span class="line"><span class="built_in">echo</span> file1 <span class="keyword">in</span> upper &gt; upper/file1</span><br></pre></td></tr></table></figure></p>
<p>现在的文件层级结构看起来是酱紫的：</p>
<p><pre><br>├── lower<br>│   ├── file1   # file1 in lower<br>│   └── file2   # file2 in lower<br>├── merged<br>├── upper<br>│   └── file1   # file1 in upper<br>└── work<br></pre><br>然后我们加载merged，让它的下层是lower，上层是upper。除此之外还需要一个workdir，据说是用来<a href="https://github.com/codelibre-net/schroot/issues/1" target="_blank" rel="external">做一些内部文件原子性操作</a>的，必须是空文件夹：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t overlay overlay -olowerdir=lower,upperdir=upper,workdir=work merged</span><br><span class="line"></span><br><span class="line">cat merged/file1   <span class="comment"># file1 in upper</span></span><br><span class="line">cat merged/file2   <span class="comment"># file2 in lower</span></span><br></pre></td></tr></table></figure></p>
<p>所以我们最终得到了类似AUFS一样的结果。测试完成，清场~<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo umount merged</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">rm -rf overlay</span><br></pre></td></tr></table></figure></p>
<p>在Linux内核3.19之后，overlay还能够支持多层lower（Multiple lower layers），这样就能更好地支持docker镜像的模型了。多层的mount命令是酱紫的：<code>mount -t overlay overlay -olowerdir=/lower1:/lower2:/lower3 /merged</code>，有兴趣的朋友可以再次升级Linux内核试试。</p>
<p>回头来看Docker官方的这幅图：<br><img src="https://docs.docker.com/engine/userguide/storagedriver/images/overlay_constructs.jpg" alt=""></p>
<p>很好地说明了OverlayFS驱动下容器和镜像的存储是怎么工作的，lower、upper和merged各自的关系。然后看看docker镜像：<br><img src="https://docs.docker.com/engine/userguide/storagedriver/images/overlay_constructs2.jpg" alt=""></p>
<p>因为目前docker支持的还不是多层存储，所以在镜像里只是用硬链接来在较低层之间共享数据。今后docker应该会利用overlay的多层技术来改善镜像各层的存储。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>ZFS和VFS由于官方都不推荐上生产我们就不试了，虽然OverlayFS也不推荐，但是它毕竟代表着未来的趋势，还是值得我们看一看的。下表列出了docker所支持的存储驱动特性对比：</p>
<table>
<thead>
<tr>
<th style="text-align:center">驱动</th>
<th style="text-align:center">联合文件系统</th>
<th style="text-align:center">写时复制</th>
<th style="text-align:center">内核</th>
<th style="text-align:center">SELinux</th>
<th style="text-align:center">上生产环境</th>
<th style="text-align:center">速度</th>
<th style="text-align:center">存储空间占用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">AUFS</td>
<td style="text-align:center">是</td>
<td style="text-align:center">文件级别</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">支持</td>
<td style="text-align:center">推荐</td>
<td style="text-align:center">快</td>
<td style="text-align:center">小</td>
</tr>
<tr>
<td style="text-align:center">Device Mapper</td>
<td style="text-align:center">不是</td>
<td style="text-align:center">块级别</td>
<td style="text-align:center">2.6.9</td>
<td style="text-align:center">支持</td>
<td style="text-align:center">有限推荐</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">较大</td>
</tr>
<tr>
<td style="text-align:center">Btrfs</td>
<td style="text-align:center">不是</td>
<td style="text-align:center">块级别</td>
<td style="text-align:center">2.6.29</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">有限推荐</td>
<td style="text-align:center">较快</td>
<td style="text-align:center">较小</td>
</tr>
<tr>
<td style="text-align:center">OverlayFS</td>
<td style="text-align:center">是</td>
<td style="text-align:center">文件级别</td>
<td style="text-align:center">3.18</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">不推荐</td>
<td style="text-align:center">快</td>
<td style="text-align:center">小</td>
</tr>
<tr>
<td style="text-align:center">ZFS</td>
<td style="text-align:center">不是</td>
<td style="text-align:center">块级别</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">不推荐</td>
<td style="text-align:center">较快</td>
<td style="text-align:center">较小</td>
</tr>
<tr>
<td style="text-align:center">VFS</td>
<td style="text-align:center">不是</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">2.4</td>
<td style="text-align:center">支持</td>
<td style="text-align:center">不推荐</td>
<td style="text-align:center">很慢</td>
<td style="text-align:center">大</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p>我们都知道docker支持多种存储驱动，默认在ubuntu上使用AUFS，其他Linux系统上使用devicemapper。这篇文章从零开始，用一些Linux的命令来使用这些不同的存储，包括AUFS、Device Mapper、Btrfs和Overlay。<br>]]>
    
    </summary>
    
      <category term="docker" scheme="http://qinghua.github.io/tags/docker/"/>
    
      <category term="storage" scheme="http://qinghua.github.io/tags/storage/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Docker的桥接网络是怎么工作的]]></title>
    <link href="http://qinghua.github.io/docker-network/"/>
    <id>http://qinghua.github.io/docker-network/</id>
    <published>2016-01-26T10:00:00.000Z</published>
    <updated>2016-01-26T13:05:08.000Z</updated>
    <content type="html"><![CDATA[<p>我们都知道docker支持多种网络，默认网络bridge是通过一个网桥进行容器间通信的。这篇文章从零开始，用一些Linux的命令来查看主机和容器间的网络通信，也顺带介绍一些网络的基本知识。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code># config.vm.network &quot;private_network&quot;, ip: &quot;192.168.33.10&quot;</code>，删掉前面的<code>#</code>注释，相当于给它分配一个<code>192.168.33.10</code>的IP。这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后运行以下命令启动并连接虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="u4E3B_u673A_u548C_u5BB9_u5668_u95F4_u7684_u7F51_u7EDC_u8FDE_u63A5"><a href="#u4E3B_u673A_u548C_u5BB9_u5668_u95F4_u7684_u7F51_u7EDC_u8FDE_u63A5" class="headerlink" title="主机和容器间的网络连接"></a>主机和容器间的网络连接</h2><p>进入虚拟机后，在vagrant主机上运行<code>ifconfig</code>，就能看到有4个网络设备及它们的IPv4地址：</p>
<ul>
<li>docker0：172.17.0.1</li>
<li>eth0：10.0.2.15</li>
<li>eth1：192.168.33.10</li>
<li>lo：127.0.0.1</li>
</ul>
<p>其中的<code>eth0</code>和<code>eth1</code>是普通的以太网卡，<code>eth1</code>就是我们解除注释的IP：<code>192.168.33.10</code>。<code>lo</code>是所谓的<a href="https://en.wikipedia.org/wiki/Loopback" target="_blank" rel="external">回环网卡</a>，每台机器都有。它将这台机器/容器绑定到<code>127.0.0.1</code>的IP上，这样子就算没有真实的网卡，也能通过这个IP访问自己，对于测试来说尤其方便。最上面的<code>docker0</code>就是我们常说的网桥。怎么知道它是个网桥呢？安装<code>bridge-utils</code>的包就能看到了：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install bridge-utils</span><br><span class="line">brctl show</span><br></pre></td></tr></table></figure></p>
<p>网桥设备就好比交换机，可以和其他的网络设备相连接，就像在其他网络设备上拉根网线到这个交换机一样。那么docker怎么使用这个网桥呢，让我们来启动一个容器看看：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name=ubuntu ubuntu:<span class="number">14.04</span> bash</span><br></pre></td></tr></table></figure></p>
<p>进入容器后，运行<code>ifconfig</code>，就能够看到有2个网络设备及它们的IPv4地址：</p>
<ul>
<li>eth0：172.17.0.2</li>
<li>lo：127.0.0.1</li>
</ul>
<p>它也有自己的<code>lo</code>，还有一块以太网卡<code>eth0</code>，目前的IP是<code>172.17.0.2</code>。使用快捷键<code>Ctrl+P</code>然后再<code>Ctrl+Q</code>，就能退出容器并保持它继续运行。在vagrant主机上运行<code>route</code>命令，可以看到类似下面这个表格：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Destination</th>
<th style="text-align:center">Gateway</th>
<th style="text-align:center">Genmask</th>
<th style="text-align:center">Flags</th>
<th style="text-align:center">Metric</th>
<th style="text-align:center">Ref</th>
<th style="text-align:center">Use</th>
<th style="text-align:center">Iface</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">default</td>
<td style="text-align:center">10.0.2.2</td>
<td style="text-align:center">0.0.0.0</td>
<td style="text-align:center">UG</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">eth0</td>
</tr>
<tr>
<td style="text-align:center">10.0.2.0</td>
<td style="text-align:center">*</td>
<td style="text-align:center">255.255.255.0</td>
<td style="text-align:center">U</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">eth0</td>
</tr>
<tr>
<td style="text-align:center">172.17.0.0</td>
<td style="text-align:center">*</td>
<td style="text-align:center">255.255.0.0</td>
<td style="text-align:center">U</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">docker0</td>
</tr>
<tr>
<td style="text-align:center">192.168.33.0</td>
<td style="text-align:center">*</td>
<td style="text-align:center">255.255.255.0</td>
<td style="text-align:center">U</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">eth1</td>
</tr>
</tbody>
</table>
<p>这个就是vagrant主机的路由表。我们重点看一下<code>172.17.0.0</code>这一行。它的Genmask为<code>255.255.0.0</code>，就意味着<code>255.255</code>对应着的IP<code>172.17</code>是网络地址，而Genmask中<code>0.0</code>对应着的IP<code>0.0</code>是主机地址。整行的意思就是当目标地址是<code>172.17.*.*</code>的时候，匹配这条路由规则。还有一种写法是<code>172.17.0.0/16</code>。当Gateway不为<code>*</code>号时，那就会路由到Gateway去，否则就路由到Iface去。刚才我们知道新容器的IP是<code>172.17.0.2</code>，所以当vagrant主机上的某个数据包的地址是这个新容器的IP时，就会匹配这条路由规则，由docker0来接受这个数据包。如果数据包的地址都不匹配这些规则，就送到<code>default</code>那一行的<code>Gateway</code>里。</p>
<p>那么docker0在接收数据包之后，又会送到哪里去呢？我们在vagrant主机再次运行<code>brctl show</code>，便能看到docker0这个网桥有所变化。它的<code>interfaces</code>里增加了一个<code>vethXXX</code>，在我的机器上叫<code>vethd6d3942</code>。在vagrant主机再次运行<code>ifconfig</code>，我们也能看到这一块新增的VETH虚拟网卡。实际上每启动一个容器，docker便会增加一个叫<code>vethXXX</code>的设备，并把它连接到docker0上，于是docker0就可以把收到的数据包发给这个VETH设备。VETH设备总是成对出现，一端进去的请求总会从peer也就是另一端出来，这样就能将一个namespace的数据发往另一个namespace，就像虫洞一样。那么现在这一端是<code>vethd6d3942</code>，它的另一端是哪儿呢？运行这个命令（记得把VETH设备名改成你自己主机上的设备名）：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -S vethd6d3942</span><br></pre></td></tr></table></figure></p>
<p>可以看到这个VETH设备的<code>peer_ifindex</code>是某个数字，在我的机器上是<code>5</code>。这个<a href="http://www.cisco.com/c/en/us/support/docs/ip/simple-network-management-protocol-snmp/28420-ifIndex-Persistence.html" target="_blank" rel="external">ifindex</a>是一个网络接口的唯一识别编号。通过<code>docker exec -it ubuntu bash</code>进入容器里，然后运行：<br><figure class="highlight sh"><figcaption><span>ubuntu container</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip link</span><br></pre></td></tr></table></figure></p>
<p>可以看到<code>5: eth0</code>，原来跨越namespace跑到容器里头来啦。这就是主机上的VETH设备能跟容器内部通信的原因。每当新启动一个容器，主机就会增加一对VETH设备，把一个连接到docker0上，另一个挂载到容器内部的eth0里。</p>
<h2 id="IP_u548Cmac_u5730_u5740_u6620_u5C04"><a href="#IP_u548Cmac_u5730_u5740_u6620_u5C04" class="headerlink" title="IP和mac地址映射"></a>IP和mac地址映射</h2><p>还有一个问题：每个网络设备都有自己的mac地址，通过ip怎么能找到它呢？在容器外运行这个命令：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arp -n</span><br></pre></td></tr></table></figure></p>
<p>我们就能看到<code>Address</code>和<code>HWaddress</code>，它们分别对应着IP地址和mac地址，这样就匹配起来了。到容器里<code>ifconfig</code>一下，看看<code>172.17.0.2</code>的mac地址，是不是和主机<code>arp -n</code>运行结果中<code>172.17.0.2</code>那行的mac地址一样呢？</p>
<h2 id="u53C2_u8003_u8D44_u6599"><a href="#u53C2_u8003_u8D44_u6599" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://www.ibm.com/developerworks/cn/linux/1310_xiawc_networkdevice/" target="_blank" rel="external">Linux上的基础网络设备详解</a>，介绍了不同的网络设备工作原理。<br><a href="http://www.oschina.net/translate/docker-network-configuration" target="_blank" rel="external">Docker网络配置</a>，从零开始配置docker的网络。<br><a href="http://vbird.dic.ksu.edu.tw/linux_server/0110network_basic.php" target="_blank" rel="external">基础网络概念</a>，来自鸟哥，深入浅出地介绍了网络的基础知识。<br><a href="http://linux.vbird.org/linux_server/0140networkcommand.php" target="_blank" rel="external">Linux常用网络命令</a>，来自鸟哥，看完了就对茫茫的网络命令有了清晰的了解。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>我们都知道docker支持多种网络，默认网络bridge是通过一个网桥进行容器间通信的。这篇文章从零开始，用一些Linux的命令来查看主机和容器间的网络通信，也顺带介绍一些网络的基本知识。<br>]]>
    
    </summary>
    
      <category term="docker" scheme="http://qinghua.github.io/tags/docker/"/>
    
      <category term="network" scheme="http://qinghua.github.io/tags/network/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[用ansible管理大规模集群]]></title>
    <link href="http://qinghua.github.io/ansible-large-scale-cluster/"/>
    <id>http://qinghua.github.io/ansible-large-scale-cluster/</id>
    <published>2016-01-21T00:23:45.000Z</published>
    <updated>2016-01-28T02:22:02.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://www.ansible.com/get-started" target="_blank" rel="external">Ansible</a>是一个配置管理工具，可以用脚本批量操作多台机器。它的特点是非常简洁，基于<a href="https://wiki.archlinux.org/index.php/Secure_Shell_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)" target="_blank" rel="external">SSH</a>，不需要安装代理。但它的缺点也很明显：效率较低，容易挂起，不那么适合大规模环境（如500台以上）。本文介绍了使用ansible管理大规模集群的几种方法。<br><a id="more"></a></p>
<h2 id="u589E_u52A0_u5E76_u884C_u8FDB_u7A0B_u6570"><a href="#u589E_u52A0_u5E76_u884C_u8FDB_u7A0B_u6570" class="headerlink" title="增加并行进程数"></a>增加并行进程数</h2><p>Ansible提供一个<a href="http://docs.ansible.com/ansible/intro_configuration.html#forks" target="_blank" rel="external">forks</a>的属性，可以设置运行并行进程数。这个值默认比较保守，只有5个并行进程。我们可以根据自己的机器性能以及网络情况来设定，很多人使用50，也有用500以上的。如果有很多机器要管理的话，可以尝试先增加这个值，看看效果。有三个地方可以设置forks的数量：</p>
<ul>
<li>环境变量：<code>export ANSIBLE_FORKS=100</code></li>
<li>ansible.cfg这个配置文件里设置：<code>forks=100</code></li>
<li>运行ansible命令时增加参数：<code>-f 100</code></li>
</ul>
<p>当机器数量比较大的时候，难免会有几台机器不能正常执行。这时候ansible会有提示<code>to retry, use: --limit @/xxx/xxx.retry</code>，把它增加到上个命令的后面就好了。</p>
<h2 id="u5F02_u6B65"><a href="#u5F02_u6B65" class="headerlink" title="异步"></a>异步</h2><p>有时候执行某个任务可能需要很长的时间，在集群规模较大的情况下慢得让人无法忍受。这时可以考虑使用<a href="http://docs.ansible.com/ansible/playbooks_async.html" target="_blank" rel="external">异步模式</a>。在<code>tasks</code>里增加<code>async</code>的属性，设成某个数字，比如60，意思就是这个任务最大运行时间不能超过60秒。也可以设成0，意思是不管任务运行多久，一直等待即可。如果没有指定<code>async</code>，则默认为同步模式。还可以设定<code>poll</code>，默认值为10，意思就是每隔10秒轮询查看结果。如果不需要查看结果，设为0就好了。还可以通过<code>register</code>和<code>async_status</code>设定暂时不查看结果，等需要的时候再查看。具体做法可以参考上面的<a href="http://docs.ansible.com/ansible/playbooks_async.html" target="_blank" rel="external">异步模式官网文档</a>，也可以看<a href="http://www.ansible.com.cn/docs/playbooks_async.html" target="_blank" rel="external">翻译的中文文档</a>。</p>
<h2 id="Pull_u6A21_u5F0F"><a href="#Pull_u6A21_u5F0F" class="headerlink" title="Pull模式"></a>Pull模式</h2><p>有些配置管理工具比如<a href="https://www.chef.io/chef/" target="_blank" rel="external">Chef</a>和<a href="https://puppetlabs.com/" target="_blank" rel="external">Puppet</a>，是基于拉模式的。所谓拉模式，是酱紫的：</p>
<ul>
<li>管理员写脚本</li>
<li>管理员上传脚本</li>
<li>agent定时取脚本（例如每隔1分钟）</li>
<li>agent运行新脚本</li>
</ul>
<p>Ansible是没有agent的，它默认基于推模式，也就是说：</p>
<ul>
<li>管理员写脚本</li>
<li>管理员运行脚本</li>
<li>ansible连接各主机运行脚本</li>
</ul>
<p>一般来说，拉模式能轻松应付大规模集群，因为每台机器都是自己去拉取脚本来完成任务。不过也有人用ansible的推模式管理着上千台机器。Ansible提供了<a href="http://docs.ansible.com/ansible/playbooks_intro.html#ansible-pull" target="_blank" rel="external">ansible-pull</a>的工具，能把它变成拉模式。官方资料不多，<a href="https://www.stavros.io/posts/automated-large-scale-deployments-ansibles-pull-mo/" target="_blank" rel="external">这篇文章</a>写得比较详细。大致思路是新建一个<a href="http://git-scm.com/" target="_blank" rel="external">git</a>的仓库，每台机器运行一个cron定时任务（扮演者agent的角色）每隔一段时间去仓库取最新脚本，然后运行之。在<a href="https://raw.githubusercontent.com/ansible/ansible/stable-2.0/CHANGELOG.md" target="_blank" rel="external">ansible 2.0</a>里<code>ansible-pull</code>也有若干改进。</p>
<h2 id="u591A_u7EA7_u8C03_u5EA6"><a href="#u591A_u7EA7_u8C03_u5EA6" class="headerlink" title="多级调度"></a>多级调度</h2><p>还有一种想法是：如果一台主机的性能只能撑100<code>forks</code>，那么10台主机应该就能撑1000台机器。将这1000台机器分区，比如A区到J区。所以由一台主机分发命令给10台主机，让它们各自运行<code>ansible-playbook</code>，而每台主机根据不同的<a href="http://docs.ansible.com/ansible/intro_inventory.html" target="_blank" rel="external">inventory</a>或者是不同的<a href="http://allandenot.com/devops/2015/01/16/ansible-with-multiple-inventory-files.html" target="_blank" rel="external">limit方式</a>来控制不同区的机器并返回结果。理论上这样的多级调度是能够撑起大规模集群的，就是脚本写起来比较麻烦，需要考虑一级主机和二级主机。</p>
<h2 id="u5176_u4ED6_u53C2_u8003_u8D44_u6599"><a href="#u5176_u4ED6_u53C2_u8003_u8D44_u6599" class="headerlink" title="其他参考资料"></a>其他参考资料</h2><p>说到底，如果运行得快，那么集群规模大一点也可以。<a href="http://www.ansible.com/blog/ansible-performance-tuning" target="_blank" rel="external">这篇文章</a>介绍了一些ansible性能调优的方法。<br><a href="https://mackerel.io/" target="_blank" rel="external">Mackerel</a>是一个监控平台。<a href="http://yuuki.hatenablog.com/entry/ansible-mackerel-1000" target="_blank" rel="external">这篇日文文章</a>介绍了使用Ansible和Mackerel API管理1000台规模集群的方法。<a href="http://www.ansible.com/tower" target="_blank" rel="external">Ansible tower</a>也提供了类似的可视化管理页面，官方出品，是不是更靠谱呢。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://www.ansible.com/get-started">Ansible</a>是一个配置管理工具，可以用脚本批量操作多台机器。它的特点是非常简洁，基于<a href="https://wiki.archlinux.org/index.php/Secure_Shell_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)">SSH</a>，不需要安装代理。但它的缺点也很明显：效率较低，容易挂起，不那么适合大规模环境（如500台以上）。本文介绍了使用ansible管理大规模集群的几种方法。<br>]]>
    
    </summary>
    
      <category term="ansible" scheme="http://qinghua.github.io/tags/ansible/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[传统企业敏捷转型纪实（二）]]></title>
    <link href="http://qinghua.github.io/waterfall-to-agile-2/"/>
    <id>http://qinghua.github.io/waterfall-to-agile-2/</id>
    <published>2016-01-20T12:11:05.000Z</published>
    <updated>2016-01-20T14:53:48.000Z</updated>
    <content type="html"><![CDATA[<p>这是启动会议结束之后一周内发生的事。我们现在有了迭代计划，一堆的story和一堆迷茫的人。接下来怎么做开发？本系列目前有两篇：</p>
<ul>
<li><a href="/waterfall-to-agile-1">传统企业敏捷转型纪实（一）</a></li>
<li><a href="/waterfall-to-agile-2">传统企业敏捷转型纪实（二）</a><a id="more"></a>
</li>
</ul>
<h2 id="u8FED_u4EE3_u5F00_u59CB"><a href="#u8FED_u4EE3_u5F00_u59CB" class="headerlink" title="迭代开始"></a>迭代开始</h2><p>上次刚刚理清楚了各组自己的需求，但是组员们并不是都完全了解。Ken先把所有人都召集起来，告诉大家：现在需求不仅仅是BA的事情了，需求不清是开发和测试的责任，大家有义务互相协作，把需求理清楚。各个PO开始讲依赖：有没有依赖于其他组的story？有没有依赖其他人（比如整个大组只有一位安全专家，可能有些story会对这个人有依赖）？PO们讲完了，有的组可能就凭空多了几张被别的组所依赖的卡，优先级还都比较高。所以需要重新安排一下迭代。计划调整完后，各个PO依次大致地给SPO讲一下自己组第一迭代的主要功能和风险，在获得SPO的认可之后，第一个迭代的计划就算确定下来。</p>
<p>然后就该每个组员认领story了。Ken要求每个story都要有对应的开发和测试人员，从新人开始认领。每个成员自己想学什么，想做什么，职业规划是什么，按照它们来决定自己要开发的story。这样的目的是激发每个人的潜能，提高团队的能力，而不仅仅是着眼于这个版本的交付。同样的，每个成员，都不仅仅是开发这个版本，而是开发一个产品。现实中，可能会出现胡乱挑卡的情况，比如说A卡可能很适合甲来做，但是乙是新人，抢先把卡挑走了，这时候就需要PO来与大家沟通，做决策。</p>
<p>落实完了每个人的工作，Ken又把大家叫到一起，问：你们对按时发布有没有信心？5分就是信心指数最高，1分最低，大家一起伸手指示意。大部分人都举4或者5，也许是无所谓，也许是还没适应一个有话就应该讲出来的环境。有个别成员伸3个指头的，就需要解释一下为什么信心不足，SPO需要当场把问题解决，尽量做到所有人都信心爆棚，起码看上去得是这样。</p>
<h2 id="u9700_u6C42_u5206_u6790"><a href="#u9700_u6C42_u5206_u6790" class="headerlink" title="需求分析"></a>需求分析</h2><p>到了具体开发阶段了，怎么做呢？第二天就是一堂需求分析的课程。大家探讨一下开发和测试怎么协作，需求应该怎么分析，测试用例应该怎么写。对于一个story，开发人员需要知道怎么测，做出来的东西由谁来用，才有能力开发。Ken引入了场景树来做需求分析。举个栗子：一个<strong>买手机</strong>的story。看起来好像需求很明确，但是具体做就会有各种问题：到底对方要的是什么样的手机？所以开发前必须搞清楚，这个story的目的是什么。买手机是内容，不是目的。用5个为什么来深挖，可能就能得到这样的目的：<strong>女朋友手机坏了，让我买个新手机</strong>。然后我们可以画出这样的图：<br><img src="/img/scene-tree-1.png" alt=""></p>
<p>第一个步骤可能就是去取款准备买手机。这个步骤可以用<strong>活动</strong>、<strong>实体</strong>、<strong>结果</strong>来建模。活动应该是动词，描述一个活动：取款。它产生了一个名词实体：人民币。校验这个实体可以得到结果，结果具有若干维度。有点晕？看图：<br><img src="/img/scene-tree-2.png" alt=""></p>
<p>取款这个活动，产生了人民币这个实体。结果的维度是金额。取完款之后，去手机店的动作，产生了手机店这个实体。结果的维度有哪家店和日期时间。到店之后，购买手机的活动产生了手机这个实体。结果的维度有品牌、型号、价格等等。这些维度越清晰，这个需求分析的质量越好。如图：<br><img src="/img/scene-tree-3.png" alt=""></p>
<p>有的朋友可能会问：除了最后得到新手机，是不是也得校验我取的款花了多少，那怎么体现在图里呢？这个还是看需求。如果必要的话，可以在购买完手机后増加一个计算余额的活动。</p>
<h2 id="u6D4B_u8BD5_u7528_u4F8B"><a href="#u6D4B_u8BD5_u7528_u4F8B" class="headerlink" title="测试用例"></a>测试用例</h2><p>画完场景图之后，就能比较容易地根据实体和维度导出测试用例来。还是以买手机为例：首先验证第一个实体：人民币。画张表格如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:right">金额</th>
<th style="text-align:center">预期结果</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">人民币</td>
<td style="text-align:right">1000</td>
<td style="text-align:center">OK</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">人民币</td>
<td style="text-align:right">0</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">银行账户余额不足</td>
</tr>
</tbody>
</table>
<p>从Given、When、Then的角度上看，再加上Given，这就是一个很具体的单元测试用例。然后是手机店：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">哪家店</th>
<th style="text-align:center">日期时间</th>
<th style="text-align:center">预期结果</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">手机店</td>
<td style="text-align:center">国美</td>
<td style="text-align:center">2016/01/20 10:00:00</td>
<td style="text-align:center">OK</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">手机店</td>
<td style="text-align:center">苏宁</td>
<td style="text-align:center">2016/01/20 22:00:00</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">下班了</td>
</tr>
<tr>
<td style="text-align:center">手机店</td>
<td style="text-align:center">苏美</td>
<td style="text-align:center">2016/01/20 10:00:00</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">没有这家店</td>
</tr>
</tbody>
</table>
<p>最后是新手机：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">品牌</th>
<th style="text-align:center">型号</th>
<th style="text-align:right">价格</th>
<th style="text-align:center">预期结果</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">新手机</td>
<td style="text-align:center">小米</td>
<td style="text-align:center">Mi Note</td>
<td style="text-align:right">1999</td>
<td style="text-align:center">OK</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">新手机</td>
<td style="text-align:center">魅族</td>
<td style="text-align:center">MX-5</td>
<td style="text-align:right">1799</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">没有这个型号</td>
</tr>
<tr>
<td style="text-align:center">新手机</td>
<td style="text-align:center">华为</td>
<td style="text-align:center">Mate8</td>
<td style="text-align:right">-3199</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">价格不正确</td>
</tr>
<tr>
<td style="text-align:center">新手机</td>
<td style="text-align:center">小魅</td>
<td style="text-align:center">MiMX</td>
<td style="text-align:right">999</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">没有这个品牌</td>
</tr>
</tbody>
</table>
<p>从上面这几张表我们也能看出来，维度越多，测试案例也就越多，所以说需求的质量就会越高。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这是启动会议结束之后一周内发生的事。我们现在有了迭代计划，一堆的story和一堆迷茫的人。接下来怎么做开发？本系列目前有两篇：</p>
<ul>
<li><a href="/waterfall-to-agile-1">传统企业敏捷转型纪实（一）</a></li>
<li><a href="/waterfall-to-agile-2">传统企业敏捷转型纪实（二）</a>]]>
    
    </summary>
    
      <category term="agile" scheme="http://qinghua.github.io/tags/agile/"/>
    
      <category term="agile" scheme="http://qinghua.github.io/categories/agile/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如果有10000台机器，你想怎么玩？（五）日志]]></title>
    <link href="http://qinghua.github.io/kubernetes-in-mesos-5/"/>
    <id>http://qinghua.github.io/kubernetes-in-mesos-5/</id>
    <published>2016-01-18T11:59:00.000Z</published>
    <updated>2016-02-03T13:27:31.000Z</updated>
    <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的集中化日志方案。日志通常是由许多文件组成，被分散地储存到不同的地方，所以需要集中化地进行日志的统计和检索。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a><a id="more"></a>
</li>
</ul>
<h2 id="u96C6_u4E2D_u5316_u65E5_u5FD7_u67B6_u6784"><a href="#u96C6_u4E2D_u5316_u65E5_u5FD7_u67B6_u6784" class="headerlink" title="集中化日志架构"></a>集中化日志架构</h2><p><a href="http://jasonwilder.com/blog/2013/07/16/centralized-logging-architecture/" target="_blank" rel="external">集中化日志架构</a>包括这几个阶段：收集、传输、存储和分析，有时候也许会涉及告警。</p>
<ul>
<li>收集：通常以代理的形式运行在各个节点上，负责收集日志。我们希望能尽可能地实时，因为当我们重现一个bug的时候，不会愿意再等上好几分钟才能看到当时的操作日志。</li>
<li>传输：把收集到的日志传给存储。这个阶段关注的是可靠性。万一日志丢失的话那可就麻烦了。</li>
<li>存储：按需选择用什么形式的存储。比如要存多久时间？要不要支持扩容？找历史数据的可能性有多大？</li>
<li>分析：不同的分析工具适用于不同的存储。这个也包含可视化的分析及报表导出等等。</li>
<li>告警：出现错误日志的时候通知运维人员。最好还能聚合相同的错误，因为作为运维来说，实在是不想看到同一个类型的错误不停地骚扰过来。</li>
</ul>
<h2 id="u4F20_u7EDF_u65E5_u5FD7_u65B9_u6848"><a href="#u4F20_u7EDF_u65E5_u5FD7_u65B9_u6848" class="headerlink" title="传统日志方案"></a>传统日志方案</h2><p>商业方案<a href="http://www.splunk.com/" target="_blank" rel="external">splunk</a>几乎拥有市面上最丰富的功能，高可用，可扩展，安全，当然很复杂也很贵。还有一个试图成为splunk的SaaS版本<a href="https://www.sumologic.com/" target="_blank" rel="external">Sumo Logic</a>，包含精简的免费版和收费版。免费方案中比较著名的有Elasticsearch公司（现在叫Elastic公司）的<a href="https://www.elastic.co/webinars/introduction-elk-stack" target="_blank" rel="external">ELK</a>和Apache的<a href="http://flume.apache.org/" target="_blank" rel="external">Flume</a>+<a href="http://kafka.apache.org/" target="_blank" rel="external">Kafka</a>+<a href="http://storm.apache.org/" target="_blank" rel="external">Storm</a>。</p>
<p>ELK是<a href="https://www.elastic.co/products/elasticsearch" target="_blank" rel="external">Elastic search</a>、<a href="https://www.elastic.co/products/logstash" target="_blank" rel="external">Logstash</a>和<a href="https://www.elastic.co/products/kibana" target="_blank" rel="external">Kibana</a>三个开源软件的组合。其中logstash可以对日志进行收集、过滤和简单处理，并将其存储到elastic search上，最终供kibana展示（和上一篇的监控很类似啊）。这套方案可以参考<a href="http://dockone.io/article/505" target="_blank" rel="external">新浪的实时日志架构</a>。这一本<a href="http://kibana.logstash.es/content/" target="_blank" rel="external">ELKstack 中文指南</a>也写得非常详细。</p>
<p>Apache的flume扮演者类似logstash的角色来收集数据，storm可以对flume采集到的数据进行实时分析。由于数据的采集和处理速度可能不一致，因此用消息中间件kafka来作为缓冲。但是kafka不可能存储所有的日志数据，所以会用其他的存储系统来负责持久化，如同样由Apache提供的<a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" target="_blank" rel="external">HDFS</a>。这套方案可以参考<a href="http://tech.meituan.com/mt-log-system-arch.html" target="_blank" rel="external">美团的日志收集系统架构</a>。如果需要对分析后的结果持久化，还可以引入<a href="https://www.mysql.com/" target="_blank" rel="external">mysql</a>等数据库。</p>
<h2 id="kubernetes_u65B9_u6848"><a href="#kubernetes_u65B9_u6848" class="headerlink" title="kubernetes方案"></a>kubernetes方案</h2><p>虽然也支持logstash，Kubernetes官方使用的是<a href="http://www.fluentd.org/" target="_blank" rel="external">fluentd</a>（有<a href="http://www.tuicool.com/articles/7FzqeeI" target="_blank" rel="external">文章</a>称logstash侧重可扩展性而fluentd侧重可靠性）。比方说我们要收集tomcat的日志，可以在tomcat的pod里增加一个<a href="https://github.com/kubernetes/contrib/tree/master/logging/fluentd-sidecar-es" target="_blank" rel="external">fluentd-sidecar-es</a>的辅助容器，指定tomcat容器的日志文件地址，再指定elastic search服务的位置（对于fluentd-sidecar-es这个特定容器来说，是写死在td-agent.conf文件里的），fluentd便会自行将日志文件发送给elastic search。至于kibana，只需指定elastic search的url就能用了。这是kibana的日志页面：<br><img src="/img/kibana.jpg" alt=""></p>
<p>还可以根据日志来配置各种图表，生成很炫的Dashboard。这个是官方的<a href="http://demo.elastic.co/packetbeat/" target="_blank" rel="external">demo</a>：<br><img src="/img/kibana-official.jpg" alt=""></p>
<p>如果日志不是写到文件系统，而是写到stdout或者stderr，那么kubernetes直接就可以用logs命令看到，就不需要这一整套了。但是一个复杂的web应用，通常还是有复杂的日志文件配置的。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这次聊聊mesos+k8s的集中化日志方案。日志通常是由许多文件组成，被分散地储存到不同的地方，所以需要集中化地进行日志的统计和检索。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a>]]>
    
    </summary>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="mesos" scheme="http://qinghua.github.io/tags/mesos/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如果有10000台机器，你想怎么玩？（四）监控]]></title>
    <link href="http://qinghua.github.io/kubernetes-in-mesos-4/"/>
    <id>http://qinghua.github.io/kubernetes-in-mesos-4/</id>
    <published>2016-01-15T11:12:59.000Z</published>
    <updated>2016-02-22T01:28:51.000Z</updated>
    <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的监控告警方案。所谓监控主要就是收集和储存主机和容器的实时数据，根据运维人员的需求展示出来的过程。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a><a id="more"></a>
</li>
</ul>
<h2 id="u6570_u636E_u91C7_u96C6"><a href="#u6570_u636E_u91C7_u96C6" class="headerlink" title="数据采集"></a>数据采集</h2><p><a href="https://github.com/google/cadvisor" target="_blank" rel="external">cAdvisor</a>由谷歌出品，可以收集主机及容器的CPU、内存、网络和存储的各项指标。它也提供了REST API以供其他程序来收集这些指标。可以很简单地用容器将它启动起来。它提供了一个页面，通过下面这幅图可以有个直观地认识：<br><img src="/img/cAdvisor.jpg" alt=""><br>kubelet集成了cAdvisor，由于kubernetes会在每个slave上启动kubelet，所以我们不用额外运行cAdvisor容器，就能够监控所有slave的主机和容器。</p>
<p>从cAdvisor提供的漂亮页面上，我们能看到某台主机及其中的容器监控数据。但是还不够，我们想要的是整个集群的数据，而非一个个单体。这时候就轮到<a href="https://github.com/kubernetes/heapster" target="_blank" rel="external">heapster</a>出场了。它支持cAdvisor和kubernetes v1.0.6及后续的版本。运行heapster需要指定两个参数：一个是用https的方式启动的kubernetes api server用来收集数据，另一个是将收集到的数据储存起来的地方，以供随时查看。</p>
<h2 id="u6570_u636E_u5B58_u50A8"><a href="#u6570_u636E_u5B58_u50A8" class="headerlink" title="数据存储"></a>数据存储</h2><p><a href="https://influxdata.com/time-series-platform/influxdb/" target="_blank" rel="external">InfluxDB</a>正是这样一个数据存储的地方。它是InfluxData公司开发的一个分布式键值<a href="http://www.infoq.com/cn/articles/database-timestamp-01" target="_blank" rel="external">时序数据库</a>，也就是说，任何数据都包含时间属性。这样可以很方便地查询到某段时间内的监控数据。举个栗子，查找5分钟前的数据：<code>WHERE time &gt; NOW() - 5m</code>。InfluxDB提供了前端页面供我们查找数据：<br><img src="/img/InfluxDB.jpg" alt=""></p>
<p>听说InfluxDB的性能一般，如果使用中遇到坑，可以试试<a href="http://opentsdb.net/" target="_blank" rel="external">OpenTSDB</a>。</p>
<h2 id="u6570_u636E_u53EF_u89C6_u5316"><a href="#u6570_u636E_u53EF_u89C6_u5316" class="headerlink" title="数据可视化"></a>数据可视化</h2><p>数据也都整合起来了，现在缺的是一个页面将这些数据显示出来。<a href="http://grafana.org/" target="_blank" rel="external">Grafana</a>是纯js开发的、拥有很炫页面的，你们喜欢的Darcula风格的前端。只要指定InfluxDB的url，它就可以轻易地将数据显示出来。看看这个页面：<br><img src="/img/Grafana.jpg" alt=""></p>
<p>heapster的数据除了传送出去保存起来，也可以被<a href="https://github.com/kubernetes/kubedash" target="_blank" rel="external">kubedash</a>所用。它也提供了监控信息的实时聚合页面，可是由于没有地方储存，看不了历史数据：<br><img src="/img/kubedash.jpg" alt=""></p>
<h2 id="u544A_u8B66"><a href="#u544A_u8B66" class="headerlink" title="告警"></a>告警</h2><p>InfluxData公司除了InfluxDB，还提供了一整套的<a href="https://influxdata.com/get-started/what-is-the-tick-stack/" target="_blank" rel="external">TICK stack</a>开源方案，其中的<a href="https://influxdata.com/time-series-platform/kapacitor/" target="_blank" rel="external">Kapacitor</a>正是一个我们需要的告警平台。它使用叫做<a href="https://docs.influxdata.com/kapacitor/v0.2/tick/" target="_blank" rel="external">TICKscript</a>的DSL，通过数据流水线来定义各种任务。通知方式除了写log、发送http请求和执行脚本，还支持<a href="https://slack.com/" target="_blank" rel="external">Slack</a>、<a href="https://www.pagerduty.com/" target="_blank" rel="external">PagerDuty</a>和<a href="https://victorops.com/" target="_blank" rel="external">VictorOps</a>。因为Kapacitor和InfluxDB都是InfluxData公司的产品，所以它们之间的无缝集成也是理所当然的。</p>
<h2 id="u5176_u4ED6_u89E3_u51B3_u65B9_u6848"><a href="#u5176_u4ED6_u89E3_u51B3_u65B9_u6848" class="headerlink" title="其他解决方案"></a>其他解决方案</h2><p><a href="https://github.com/prometheus/prometheus" target="_blank" rel="external">Prometheus</a>是一个监控系统解决方案，包含了数据采集、时序数据库、UI可视化、告警等诸多功能。它的特点是可以实现多纬度的监控，在对比不同实例的监控数据图上有优势。它还有许多的<a href="https://prometheus.io/docs/instrumenting/exporters/" target="_blank" rel="external">exporter</a>可以很方便地从许多第三方应用中导出数据，如Apache、AWS、Redis等，也支持Kubernetes和Kubernetes-Mesos。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这次聊聊mesos+k8s的监控告警方案。所谓监控主要就是收集和储存主机和容器的实时数据，根据运维人员的需求展示出来的过程。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a>]]>
    
    </summary>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="mesos" scheme="http://qinghua.github.io/tags/mesos/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[传统企业敏捷转型纪实（一）]]></title>
    <link href="http://qinghua.github.io/waterfall-to-agile-1/"/>
    <id>http://qinghua.github.io/waterfall-to-agile-1/</id>
    <published>2016-01-13T13:39:15.000Z</published>
    <updated>2016-01-20T14:51:17.000Z</updated>
    <content type="html"><![CDATA[<p>这是发生在某大型企业中的某个部门的事儿。他们有六七十名成员，运用瀑布开发模式，三个月发布一次内部产品。部署的过程长达一周，质量堪忧，交付日难以保证。于是请来一位很有经验的敏捷教练Ken，来帮助这个部门做敏捷转型，解决问题。本文的主要内容是在培训完敏捷的基本思想后，<a href="http://scaledagileframework.com/pi-planning/" target="_blank" rel="external">PI Planning</a>（启动会议）上发生的事。本系列目前有两篇：</p>
<ul>
<li><a href="/waterfall-to-agile-1">传统企业敏捷转型纪实（一）</a></li>
<li><a href="/waterfall-to-agile-2">传统企业敏捷转型纪实（二）</a><a id="more"></a>
</li>
</ul>
<h2 id="u56E2_u961F_u6784_u5EFA"><a href="#u56E2_u961F_u6784_u5EFA" class="headerlink" title="团队构建"></a>团队构建</h2><p>这个部门由四个团队组成，各自主管产品的一部分。每个团队都有对交付负责的人，称为PO（Product Owner）。Ken的第一步是让所有团队选出一名SPO（Super Product Owner），来对整个产品负责。SPO不做兼职，负责解决各种问题，其最重要的任务是做决策。而对于一款探索性的产品来说，决策是没有对错之分的，只是它需要靠执行力和客户反馈来修正。PO们需要力挺SPO，充分信任SPO的决策，并以团队的执行力来保证决策的执行。所以有个PO与SPO隔空喊话效忠的过程。</p>
<p>这个部门比较特殊的地方在于，大部分成员属于两个外包公司，其中有不少新人。大家对敏捷都没有什么概念，对要做的事也心有疑虑。有鉴于此，Ken让两个外包公司的人各选出1名leader来当<a href="http://scaledagileframework.com/scrum-master/" target="_blank" rel="external">SM</a>（scrum master）。如果团队成员士气低落，不管任何原因，都可以找SM沟通。如果涉及到甲方公司，便由SM来沟通PO处理。这样做的目的是让每个团队成员都有渠道摆脱自己受到的干扰，增加工作效率。团队成员也需要在团队中建立起自己的人脉，好让自己遇到问题时容易找人帮忙。SM主要负责沟通，PO带领团队前进。这样的构建适用于200~300人以下的团队。</p>
<h2 id="u4E86_u89E3_u4EA7_u54C1"><a href="#u4E86_u89E3_u4EA7_u54C1" class="headerlink" title="了解产品"></a>了解产品</h2><p>要做好产品，首先需要让团队成员理解产品，建立共识。如果只见树木不见森林，那么人人都只是开发自己的那一亩三分地，根本无从得知自己的工作在整个产品中处于什么样的位置，那怎么能做好这个产品呢？现实中，这个产品有着非常复杂的架构，甚至没有一个人能完整地解释整个架构图。Ken建议SPO找几个资深人员，专门抽出一天时间给所有人都讲清楚架构。这很重要，如果你连孩子是男是女都不知道，怎么抚养ta？团队成员需要非常了解产品，而不仅仅是某个需求或者某个版本。要做产品，不是为了做事而做事。同时，Ken也建议所有成员都花时间在架构课之前自学其中的一些重要技术，以便让自己能够在架构课上更加清除对方究竟在讲什么。也就是预习，省的回头听不懂。</p>
<h2 id="u65E5_u7A0B_u7BA1_u7406"><a href="#u65E5_u7A0B_u7BA1_u7406" class="headerlink" title="日程管理"></a>日程管理</h2><p>接下来用倒推法确定迭代截止日。假设产品5月底上线，需要提前一个月也就是4月底出beta版。需要3周的SIT时间，所以差不多是4月8号所有迭代完成。如果每两周一个迭代，从下周一算第一迭代开始，扣去春节，那么正好有5个迭代。如何能保证按时交付呢？需求可能发生变化，环境可能有问题，心情可能不太好影响了效率。迭代的意义在于提早发现风险。所以每个团队成员遇到问题时，需要尽快把这个问题暴露出来，否则，按时完成是不太可能的。</p>
<h2 id="u4F30_u7B97_u5DE5_u4F5C_u91CF"><a href="#u4F30_u7B97_u5DE5_u4F5C_u91CF" class="headerlink" title="估算工作量"></a>估算工作量</h2><p>因为是从瀑布开发模式转过来的，所以现在每个团队手里都有一大堆需求。这里使用估点的方式来估算每个需求的工作量，转化成各个<a href="https://en.wikipedia.org/wiki/User_story" target="_blank" rel="external">User Story</a>。先找一个清晰的需求，最好半天就能开发完成，再半天测试完成。对这个story估点为1。以其为基准，其他的story与它相比较，得到其他story的估点。估的点数是在一个斐波那契数列里的：<a href="https://en.wikipedia.org/wiki/Planning_poker" target="_blank" rel="external">1，2，3，5，8，13，20，40，100</a>（当然后面几个不是）。例如基准story是3点，如果一个story感觉比它难上两个等级，那这个story应该是8个点。如果比它容易一个等级，那应该是2个点。如果难上4个等级呢？因为估点是个主观的过程，而且是比较不精确的。所以当差别很大的时候误差也会很大的。20，40，100这三个数虽然不是斐波那契数列，但也有它的含义。如果一个story只有一行字，谁也说不清它包含着什么，那就是100点。如果知道一部分，那就是40点。如果知道得更多，那就是20点。当然这也是非常主观且粗糙的，但是当你看到这3个数的时候自然就知道应该先把需求搞清楚。</p>
<p>值得一提的是如果一个story估点为8，并不意味着它需要在整8天的时候做完。这个story和其他story一样，需要在最短的时间内有质量地完成。8代表着这个story的复杂度，或者说它是一个风险识别指标。如果做这个story的时候出了问题，需要开发人员尽快把这个问题暴露出来，就像上面讲的那样。而PO、SPO应该要想办法解决这个问题。如果问题超出SPO的权力，那就需要SPO的决策–可以选择不做这个story，或者只做一部分，或者绕过去。估点往往需要很长的时间。为了效率起见，当<a href="https://en.wikipedia.org/wiki/Business_analyst" target="_blank" rel="external">Business Analyst</a>讲完story时，团队成员应该有意识地思考：这个story有什么业务价值？是必须要做的吗？只有必要的story才估点。估点时新人由于还不熟悉背景，可以仅旁观不参与。参与的成员们同时伸手指表示点数，如果一样就记下点数，跳到下一个story。否则，大家就需要解释为什么自己估的点数是这个数，最后由PO拍脑袋做决策。</p>
<p>估点是个很费时，但又很重要的事情，所以先由一个团队演示几个story，其他团队观看，等大家都了解了，再由所有团队自行估点。</p>
<h2 id="u8FED_u4EE3_u8BA1_u5212"><a href="#u8FED_u4EE3_u8BA1_u5212" class="headerlink" title="迭代计划"></a>迭代计划</h2><p>最后就是安排工作量了。先要确认所有人力是否可以100%地投入。资深成员可以算全职，新人算半职，资深成员但还兼其他工作安排的也算半职。假如说最后我们得到了这样的数：</p>
<ul>
<li>全职开发：3个</li>
<li>半职开发：4个</li>
<li>全职测试：1个</li>
<li>半职测试：2个</li>
</ul>
<p>如果第一迭代有10个工作日，那么我们就能计算出来最大工作量：<code>(3+1)×10+(4+2)×10÷2=70</code>。由于是春节，可能请假会比较多，扣去请假天数，也许得到60点。然后是打折，由各组PO和SPO商量，得到一个折扣。这个折扣可能是：我们组对外部环境依赖很多，第一迭代刚开始效率会很低，春节假期效率不高，团队成员都是单身需要相亲无心干活等等等等。比如说第一迭代打个7折，就能得到合理工作量：<code>60×7=42</code>。由此，我们就得到最大工作量和合理工作量。算出各个迭代的工作量，把它们写在显眼处。最后将先前估好点的story按优先级及依赖顺序往里安排，点数到合理工作量即可。至此，一个看似合理的、由团队成员做出的计划便产生了。项目启动会议完成。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这是发生在某大型企业中的某个部门的事儿。他们有六七十名成员，运用瀑布开发模式，三个月发布一次内部产品。部署的过程长达一周，质量堪忧，交付日难以保证。于是请来一位很有经验的敏捷教练Ken，来帮助这个部门做敏捷转型，解决问题。本文的主要内容是在培训完敏捷的基本思想后，<a href="http://scaledagileframework.com/pi-planning/">PI Planning</a>（启动会议）上发生的事。本系列目前有两篇：</p>
<ul>
<li><a href="/waterfall-to-agile-1">传统企业敏捷转型纪实（一）</a></li>
<li><a href="/waterfall-to-agile-2">传统企业敏捷转型纪实（二）</a>]]>
    
    </summary>
    
      <category term="agile" scheme="http://qinghua.github.io/tags/agile/"/>
    
      <category term="agile" scheme="http://qinghua.github.io/categories/agile/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[基于docker的MySQL主从复制（replication）]]></title>
    <link href="http://qinghua.github.io/mysql-replication/"/>
    <id>http://qinghua.github.io/mysql-replication/</id>
    <published>2016-01-11T02:20:35.000Z</published>
    <updated>2016-01-16T03:51:33.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://dev.mysql.com/doc/refman/5.7/en/replication.html" target="_blank" rel="external">MySQL复制技术</a>可以异步地将主数据库的数据同步到从数据库。根据设置可以复制所有数据库、指定数据库甚至可以指定表。本文的主要内容是怎样用docker从零开始搭建mysql主从复制环境，支持binary logging方式和GTIDs方式。<br><a id="more"></a></p>
<p>MySQL 5.7支持多种复制方法。<a href="http://dev.mysql.com/doc/refman/5.7/en/replication-howto.html" target="_blank" rel="external">传统的方法</a>是master使用binary logging，slave复制并重放日志中的事件。<a href="http://dev.mysql.com/doc/refman/5.7/en/replication-gtids.html" target="_blank" rel="external">另一种方法</a>是利用GTIDs（global transaction identifiers）将所有未执行的事务在slave重放。</p>
<h2 id="binary_logging_u65B9_u5F0F"><a href="#binary_logging_u65B9_u5F0F" class="headerlink" title="binary logging方式"></a>binary logging方式</h2><p>接下来先用传统的方法试一下。使用<a href="https://hub.docker.com/r/library/mysql/" target="_blank" rel="external">MySQL 5.7镜像</a>，将<code>/etc/mysql/conf.d/</code>复制到主机，然后修改配置：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> --name=mysql mysql:<span class="number">5.7</span></span><br><span class="line">docker cp mysql:/etc/mysql/my.cnf my.cnf</span><br></pre></td></tr></table></figure></p>
<p>master的配置在<code>my.cnf</code>文件中是这样的，改完后另存为<code>/vagrant/mysql/mymaster.cnf</code>：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[mysqld]</span></span><br><span class="line"><span class="setting">log-bin=<span class="value">mysql-bin # 使用binary logging，mysql-bin是log文件名的前缀</span></span></span><br><span class="line"><span class="setting">server-id=<span class="value"><span class="number">1</span>       # 唯一服务器ID，非<span class="number">0</span>整数，不能和其他服务器的server-id重复</span></span></span><br></pre></td></tr></table></figure></p>
<p>slave的配置就更简单了，改完后另存为<code>/vagrant/mysql/myslave.cnf</code>：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[mysqld]</span></span><br><span class="line"><span class="setting">server-id=<span class="value"><span class="number">2</span>       # 唯一服务器ID，非<span class="number">0</span>整数，不能和其他服务器的server-id重复</span></span></span><br></pre></td></tr></table></figure></p>
<p>slave没有必要非得用binary logging，但是如果用了，除了binary logging带来的好处以外，还能使这个slave成为其他slave的master。现在我们重新启动mysql master和slave：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> mysql</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --net=host \</span><br><span class="line">  --name=master \</span><br><span class="line">  <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">  -v /vagrant/mysql/mymaster.cnf:/etc/mysql/my.cnf \</span><br><span class="line">  mysql:<span class="number">5.7</span></span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=slave \</span><br><span class="line">  <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">  -v /vagrant/mysql/myslave.cnf:/etc/mysql/my.cnf \</span><br><span class="line">  mysql:<span class="number">5.7</span></span><br></pre></td></tr></table></figure></p>
<p>在master创建一个复制用的用户：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it master mysql -uroot -p123456</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">'repl'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'123456'</span>;</span>       <span class="comment">-- '%'意味着所有的终端都可以用这个用户登录</span></span><br><span class="line"><span class="operator"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>,<span class="keyword">REPLICATION</span> <span class="keyword">SLAVE</span> <span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'repl'</span>@<span class="string">'%'</span>;</span> <span class="comment">-- SELECT权限是为了让repl可以读取到数据，生产环境建议创建另一个用户</span></span><br></pre></td></tr></table></figure>
<p>在slave用新创建的用户连接master（记得把MASTER_HOST改为自己的主机IP）：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it slave mysql -uroot -p123456</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CHANGE</span> <span class="keyword">MASTER</span> <span class="keyword">TO</span> MASTER_HOST=<span class="string">'192.168.33.32'</span>, MASTER_USER=<span class="string">'repl'</span>, MASTER_PASSWORD=<span class="string">'123456'</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">START</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SHOW</span> <span class="keyword">SLAVE</span> <span class="keyword">STATUS</span>\<span class="keyword">G</span>       <span class="comment">-- \G用来代替";"，能把查询结果按键值的方式显示</span></span></span><br></pre></td></tr></table></figure>
<p>如果一切正常，应该在<code>Last_Error</code>中能看到<code>Can&#39;t create database &#39;mysql&#39;</code>的错误。这是因为slave也是像master一样正常地启动，mysql数据库已经被创建了，所以不能再将master的mysql数据库同步过来。有4种解决办法：</p>
<ol>
<li><p>通过在slave上运行SQL来跳过这个复制操作的方式来实现。在slave上运行：</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">STOP</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SET</span> <span class="keyword">GLOBAL</span> SQL_SLAVE_SKIP_COUNTER = <span class="number">1</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">START</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SHOW</span> <span class="keyword">SLAVE</span> <span class="keyword">STATUS</span>\<span class="keyword">G</span></span></span><br></pre></td></tr></table></figure>
<p> 不出意外的话，上面的错误应该已经换成了其他错误（例如：<code>Duplicate entry &#39;row_evaluate_cost&#39; for key &#39;PRIMARY&#39;</code>），都是跟mysql这个数据库有关。反复运行上面的SQL直至错误消失。</p>
</li>
<li><p>通过在slave上面配置log文件名及位置的方式来实现。在master上运行：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it master mysql -uroot -p123456</span><br></pre></td></tr></table></figure>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">FLUSH</span> <span class="keyword">TABLES</span> <span class="keyword">WITH</span> <span class="keyword">READ</span> <span class="keyword">LOCK</span>;</span> <span class="comment">--防止有人对master做更新操作使Position持续变化，先锁表</span></span><br><span class="line"><span class="operator"><span class="keyword">SHOW</span> <span class="keyword">MASTER</span> <span class="keyword">STATUS</span>\<span class="keyword">G</span></span></span><br></pre></td></tr></table></figure>
<p> 可以看到<code>File: mysql-bin.000003</code>和<code>Position: 154</code>这样的行。删掉这个旧的slave并重新启动一个新的容器，然后运行：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> slave</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=slave \</span><br><span class="line">  <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">  -v /vagrant/mysql/myslave.cnf:/etc/mysql/my.cnf \</span><br><span class="line">  mysql:<span class="number">5.7</span></span><br><span class="line">docker <span class="built_in">exec</span> -it slave mysql -uroot -p123456</span><br></pre></td></tr></table></figure>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">STOP</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">CHANGE</span> <span class="keyword">MASTER</span> <span class="keyword">TO</span> MASTER_HOST=<span class="string">'192.168.33.32'</span>, MASTER_USER=<span class="string">'repl'</span>, MASTER_PASSWORD=<span class="string">'123456'</span>, MASTER_LOG_FILE=<span class="string">'mysql-bin.000003'</span>, MASTER_LOG_POS=<span class="number">154</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">START</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SHOW</span> <span class="keyword">SLAVE</span> <span class="keyword">STATUS</span>\<span class="keyword">G</span></span></span><br></pre></td></tr></table></figure>
<p> 我们将会看到<code>Slave_IO_Running: Yes</code>和<code>Slave_SQL_Running: Yes</code>。这两项说明我们的slave已经成功启动了。如果先前锁了master的表，记得在master上运行<code>UNLOCK TABLES;</code>来恢复。</p>
</li>
<li><p>通过不记录<code>mysql</code>数据库binary logging的方式来实现。既然<code>mysql</code>不在binary logging里，那它也无法被同步到slave上。在<code>/vagrant/mysql/mymaster.cnf</code>里增加一个参数，如果有多个数据库，可以复制多行：</p>
 <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="setting">binlog-ignore-db=<span class="value">mysql</span></span></span><br></pre></td></tr></table></figure>
<p> 删除master和slave容器然后再重新创建之：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> master</span><br><span class="line">docker rm <span class="operator">-f</span> slave</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --net=host \</span><br><span class="line">  --name=master \</span><br><span class="line">  <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">  -v /vagrant/mysql/mymaster.cnf:/etc/mysql/my.cnf \</span><br><span class="line">  mysql:<span class="number">5.7</span></span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=slave \</span><br><span class="line">  <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">  -v /vagrant/mysql/myslave.cnf:/etc/mysql/my.cnf \</span><br><span class="line">  mysql:<span class="number">5.7</span></span><br></pre></td></tr></table></figure>
<p> 然后根据上文所述在master创建一个复制用的用户并在slave用新创建的用户连接master，最后观察<code>Slave_IO_Running</code>和<code>Slave_SQL_Running</code>。</p>
</li>
<li><p>通过不复制<code>mysql</code>数据库binary logging的方式来实现。这种方式很类似上面一种方法，只不过配置在slave端而非master端而已。在<code>/vagrant/mysql/myslave.cnf</code>里增加一个参数，删除master和slave容器然后再重新创建之：</p>
 <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="setting">replicate-ignore-db=<span class="value">mysql</span></span></span><br></pre></td></tr></table></figure>
<p> 其余操作同方法3。</p>
</li>
</ol>
<p>既然slave已经成功启动了，我们便可以测试一下。看看在master上创建一个新数据库是否能同步到slave上：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> master mysql -uroot -p123456 <span class="operator">-e</span> <span class="string">"CREATE DATABASE ggg"</span></span><br><span class="line">docker <span class="built_in">exec</span> slave mysql -uroot -p123456 <span class="operator">-e</span> <span class="string">"SHOW DATABASES"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="GTIDs_u65B9_u5F0F"><a href="#GTIDs_u65B9_u5F0F" class="headerlink" title="GTIDs方式"></a>GTIDs方式</h2><p>下面介绍一下GTIDs方式的主从复制方法。需要修改<code>/vagrant/mysql/mymaster.cnf</code>：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[mysqld]</span></span><br><span class="line"><span class="setting">log-bin=<span class="value">mysql-bin</span></span></span><br><span class="line"><span class="setting">server-id=<span class="value"><span class="number">1</span></span></span></span><br><span class="line"><span class="setting">gtid-mode=<span class="value"><span class="keyword">on</span></span></span></span><br><span class="line"><span class="setting">enforce-gtid-consistency=<span class="value"><span class="keyword">true</span></span></span></span><br></pre></td></tr></table></figure></p>
<p>还需要修改<code>/vagrant/mysql/myslave.cnf</code>（MySQL 5.7.4及之前的版本还需要开启log-bin）：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[mysqld]</span></span><br><span class="line"><span class="setting">server-id=<span class="value"><span class="number">2</span></span></span></span><br><span class="line"><span class="setting">replicate-ignore-db=<span class="value">mysql</span></span></span><br><span class="line"><span class="setting">gtid-mode=<span class="value"><span class="keyword">on</span></span></span></span><br><span class="line"><span class="setting">enforce-gtid-consistency=<span class="value"><span class="keyword">true</span></span></span></span><br></pre></td></tr></table></figure></p>
<p>启动容器，创建复制的用户都和上面一样，在slave增加<code>MASTER_AUTO_POSITION</code>参数来连接master（记得把MASTER_HOST改为自己的主机IP）：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it slave mysql -uroot -p123456</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CHANGE</span> <span class="keyword">MASTER</span> <span class="keyword">TO</span> MASTER_HOST=<span class="string">'192.168.33.32'</span>, MASTER_USER=<span class="string">'repl'</span>, MASTER_PASSWORD=<span class="string">'123456'</span>, MASTER_AUTO_POSITION=<span class="number">1</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">START</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SHOW</span> <span class="keyword">SLAVE</span> <span class="keyword">STATUS</span>\<span class="keyword">G</span></span></span><br></pre></td></tr></table></figure>
<p>搞定！这样就不需要用到<code>MASTER_LOG_FILE</code>和<code>MASTER_LOG_POS</code>了，省事儿啊。在<code>START SLAVE</code>之前master的其它更新也都会被同步到slave。</p>
<h2 id="u5176_u4ED6_u6280_u5DE7"><a href="#u5176_u4ED6_u6280_u5DE7" class="headerlink" title="其他技巧"></a>其他技巧</h2><p>最后再介绍一些实用技巧：</p>
<ol>
<li>如果master已经有数据了，怎么新增slave：可以先把master的数据导入到slave，再启动slave。具体可以参考<a href="http://dev.mysql.com/doc/refman/5.7/en/replication-setup-slaves.html#replication-howto-existingdata" target="_blank" rel="external">这里</a>。</li>
<li>如果已经有主从复制了，怎么增加slave：思路同上，不过不需要使用master的数据，直接用已有的slave数据就可以了。不需要停止master，新slave使用新的<code>server-id</code>。具体可以参考<a href="http://dev.mysql.com/doc/refman/5.7/en/replication-howto-additionalslaves.html" target="_blank" rel="external">这里</a>。</li>
<li><p>slave设置只读操作：在<code>/vagrant/mysql/myslave.cnf</code>里增加参数即可。    <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="setting">read-only=<span class="value"><span class="number">1</span>       # 除非有SUPER权限，否则只读</span></span></span><br><span class="line"><span class="setting">super-read-only=<span class="value"><span class="number">1</span> # SUPER权限也是只读</span></span></span><br></pre></td></tr></table></figure></p>
</li>
<li><p>前面介绍的都是主从，如果需要slave也能同步到master就要设置主主复制：也就是说反过来再做一遍。</p>
</li>
<li>当slave比较多得时候，master的负载可能会成为问题。可以用主从多级复制：以slave为master来再引入新的slave。</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://dev.mysql.com/doc/refman/5.7/en/replication.html">MySQL复制技术</a>可以异步地将主数据库的数据同步到从数据库。根据设置可以复制所有数据库、指定数据库甚至可以指定表。本文的主要内容是怎样用docker从零开始搭建mysql主从复制环境，支持binary logging方式和GTIDs方式。<br>]]>
    
    </summary>
    
      <category term="mysql" scheme="http://qinghua.github.io/tags/mysql/"/>
    
      <category term="db" scheme="http://qinghua.github.io/categories/db/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如果有10000台机器，你想怎么玩？（三）持久化]]></title>
    <link href="http://qinghua.github.io/kubernetes-in-mesos-3/"/>
    <id>http://qinghua.github.io/kubernetes-in-mesos-3/</id>
    <published>2016-01-05T12:06:33.000Z</published>
    <updated>2016-02-03T13:27:19.000Z</updated>
    <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的持久化问题。如果我用容器跑一个数据库，比如mysql，我关心的是数据保存在哪里。这样万一这个容器发生意外，起码我的数据还在，还可以东山再起。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a><a id="more"></a>
</li>
</ul>
<p>为了方便部署和升级，我们目前把mesos slave跑在容器里。如果我有一个网络存储比如nfs，ceph之类的，当我命令k8s给我跑一个mysql pod，存储挂载到ceph上的时候，k8s就会先找一个mesos slave，让它挂载远端的ceph。由于mesos slave是在容器里，所以挂载点也在这个容器里，姑且把这个路径叫做<code>/tmp/mesos/slaves/20160105-xxx</code>。然后mysql容器也启动了，挂载了<code>/tmp/mesos/slaves/20160105-xxx</code>–可惜的是这个路径是主机的路径，并不是mesos slave容器里的路径，所以它并不能把数据同步到远端的ceph存储去。持久化失败。</p>
<p>有三种方案可以解决持久化的问题。第一个方案：如果我们要继续使用容器化的mesos slave，有一个办法是提前在主机上挂载远端存储。这样的话，mysql容器就可以配置成hostPath的方式，直接挂载这个主机路径，这样就能把数据同步到远端去。这么做是可行的，但是也有不少缺点。首先，因为不知道mysql容器会在哪台机器上运行，所以不得不在所有的机器上都预加载，这样做就失去了动态性，可能引发更多的问题。其次，不是所有类型的存储都可以被很多机器加载。比如ceph的rbd存储就(最好)只能被一台机器加载。还有，何时卸载？如何卸载？存储太多的时候如何管理？这些都是问题。另一个办法是使用kubernetes的<a href="https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/container-environment.md#container-hooks" target="_blank" rel="external">container hook</a>。目前支持<a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_lifecycle" target="_blank" rel="external">postStart和preStop</a>两个时点。可惜mesos slave容器里的mount并不能为外部所用。直接在mysql容器去做mount理论能行，但是需要主机的root权限，或者是hook挂上http请求去外部挂载，不管怎样都相当于重新自己来一套，并不划算。</p>
<p>第二个方案：还是容器化的mesos slave，但是使用<a href="https://hub.docker.com/r/mesosphere/mesos-slave-dind/" target="_blank" rel="external">docker in docker</a>。这种容器方案会把mysql容器运行在mesos slave容器里面，而不像第一种那样把它运行在与mesos slave并列的主机级别。所以mysql使用的存储自然而然就落到了mesos slave容器里面，而这个路径正是加载了远端ceph的地方。这个方案相对来说在操作上也挺简单，仅仅是换个mesos slave dind的镜像而已。它的缺点也正是由于新容器会运行在mesos slave dind容器里，从而导致这个主容器里面可能同时运行许多个从容器，这样就有点儿把容器当虚拟机的意思了，不是最佳实践。另外在实际操作上还出现了新的问题：比如kubernetes使用rbd方式作为volumn的时候，mesos slave会尝试将一个rbd镜像映射成一个设备<code>/dev/rbd1</code>。这个设备就会跑到主机上而非mesos slave dind容器里，从而使我们不得不将主机的<code>/dev</code>也挂载到mesos slave dind容器里。而这样的操作又会带来更多的问题：比如容器删除时提示<code>device or resource busy</code>，从而无法轻易释放<code>/var/lib/docker/aufs</code>的磁盘资源等等。鉴于继续前行可能会碰到更多更深的坑，我们主动放弃了这个方案，但它的前途也有可能是光明的。</p>
<p>第三个方案：放弃mesos slave的容器化。回顾问题的根源，一切的一切都是因为引入mesos slave的容器造成的。如果把mesos slave还原成系统进程，那么这一堆存储问题都将不复存在。我们仍然有其他手段来实现mesos slave部署和升级的便利性，如自动化脚本、数据用容器等。虽然这样可能引入更大的部署工作量，但这可能是针对这个问题来说更加正统的解决方案。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这次聊聊mesos+k8s的持久化问题。如果我用容器跑一个数据库，比如mysql，我关心的是数据保存在哪里。这样万一这个容器发生意外，起码我的数据还在，还可以东山再起。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a>]]>
    
    </summary>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="mesos" scheme="http://qinghua.github.io/tags/mesos/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[用容器轻松搭建ceph实验环境]]></title>
    <link href="http://qinghua.github.io/ceph-demo/"/>
    <id>http://qinghua.github.io/ceph-demo/</id>
    <published>2016-01-02T01:07:33.000Z</published>
    <updated>2016-01-28T06:38:15.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://ceph.com/" target="_blank" rel="external">Ceph</a>是一个高性能的PB级分布式文件系统。它能够在一个系统中提供对象存储、块存储和文件存储。本文的主要内容是怎样用docker从零开始搭建ceph环境，并分别以cephfs和rbd的方式加载它。<br><a id="more"></a></p>
<p>这是ceph的模块架构图：<br><img src="http://docs.ceph.com/docs/master/_images/stack.png" alt=""><br>最底层的RADOS提供了对象存储的功能，是ceph的根基所在。所有的其他功能都是在RADOS之上构建的。LIBRADOS看名字就能猜到，它提供了一系列语言的接口，可以直接访问RADOS。RADOSGW基于LIBRADOS实现了REST的接口并兼容S3和Swift。RBD也基于LIBRADOS提供了块存储。最后是CEPH FS直接基于RADOS实现了文件存储系统。想要详细了解它的朋友可以看看<a href="http://www.wzxue.com/why-ceph-and-how-to-use-ceph/" target="_blank" rel="external">这篇文章</a>，把ceph介绍得很清楚。</p>
<h2 id="cephfs_u65B9_u5F0F"><a href="#cephfs_u65B9_u5F0F" class="headerlink" title="cephfs方式"></a>cephfs方式</h2><p>Talk is cheap，让我们来看看如何用最简单的方式来搭建一个ceph环境吧。Ceph提供了一个<a href="https://hub.docker.com/r/ceph/demo/" target="_blank" rel="external">deph/demo</a>的docker镜像来给我们做实验，注意<strong>别在产品环境用它（THIS CONTAINER IS NOT RECOMMENDED FOR PRODUCTION USAGE）</strong>。只要装好了docker，跑起来是很容易的：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host -v /etc/ceph:/etc/ceph <span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">0.20</span> <span class="operator">-e</span> CEPH_NETWORK=<span class="number">192.168</span>.<span class="number">0.0</span>/<span class="number">24</span> --name=ceph ceph/demo</span><br></pre></td></tr></table></figure></p>
<p>上面的<code>MON_IP</code>可以填写运行这个镜像的机器IP，<code>CEPH_NETWORK</code>填写允许访问这个ceph的IP范围。启动之后，由于挂载了宿主机的<code>/etc/ceph</code>，这个文件夹里面会生成几个配置文件。其中有一个叫<code>ceph.client.admin.keyring</code>的文件里面有一个<code>key</code>，作为cephfs加载的时候认证会用到。</p>
<p>直接作为cephfs来加载就是一句话的事情：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t ceph -o name=admin,secret=AEAq5XtW5SLsARBAAh6kwpBmGVVjUwPQmZeuik== <span class="number">192.168</span>.<span class="number">0.20</span>:/ /mnt/cephfs</span><br></pre></td></tr></table></figure></p>
<p>用的时候记得事先创建好<code>/mnt/cephfs/</code>这个文件夹，替换<code>secret</code>为你自己的<code>key</code>，再改成用你的ceph服务器ip就好了。</p>
<h2 id="rbd_u65B9_u5F0F"><a href="#rbd_u65B9_u5F0F" class="headerlink" title="rbd方式"></a>rbd方式</h2><p>还有一种方式是作为rbd来加载。这边需要啰嗦几句rbd的模型：最外层是<a href="http://docs.ceph.com/docs/master/rados/operations/pools/" target="_blank" rel="external">pool</a>，相当于一块磁盘，默认的pool名字叫做rbd。每个pool里面可以有多个image，相当于文件夹。每个image可以映射成一个块设备，有了设备就可以加载它。下面我们来尝试一下。如果打算用另一台机器，需要先把<code>/etc/ceph</code>这个文件夹复制过去，这个文件夹里面包含了ceph的连接信息。为了运行ceph的命令，我们还需要安装<code>ceph-common</code>，自己选一个命令吧：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt-get install -y ceph-common</span><br><span class="line">yum install -y ceph-common</span><br></pre></td></tr></table></figure></p>
<p>准备工作做完了，我们首先创建一个名为ggg的pool：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create ggg <span class="number">128</span></span><br></pre></td></tr></table></figure></p>
<p>128代表<a href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/" target="_blank" rel="external">placement-group</a>的数量。每个pg都是一个虚拟节点，将自己的数据存在不同的位置。这样一旦存储挂了，pg就会选择新的存储，从而保证了自动高可用。运行这个命令就可以看到现在系统中的所有pool：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd lspools</span><br></pre></td></tr></table></figure></p>
<p>然后在ggg这个pool里创建一个名为qqq的image：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd create ggg/qqq --size <span class="number">1024</span></span><br></pre></td></tr></table></figure></p>
<p>size的单位是MB，所以这个qqq image的大小为1GB。要是这条命令一直没有响应，试着重启一下ceph/demo容器<code>docker restart ceph</code>，说了这不适合用于生产环境…运行下列命令可以看到ggg的pool中的所有image和查看qqq image的详细信息：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbd ls ggg</span><br><span class="line">rbd info ggg/qqq</span><br></pre></td></tr></table></figure></p>
<p>接下来要把qqq image映射到块设备中，可能需要root权限：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rbd map ggg/qqq</span><br></pre></td></tr></table></figure></p>
<p>运行这个命令就可以看到映射到哪个设备去了，我的是<code>/dev/rbd1</code>：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd showmapped</span><br></pre></td></tr></table></figure></p>
<p>格式化之：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mkfs.ext4 -m0 /dev/rbd1</span><br></pre></td></tr></table></figure></p>
<p>然后就可以加载了！里面应该有一个<code>lost+found</code>的文件夹：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /mnt/rbd/qqq</span><br><span class="line">sudo mount /dev/rbd1 /mnt/rbd/qqq</span><br><span class="line">ls /mnt/rbd/qqq/</span><br></pre></td></tr></table></figure></p>
<h2 id="u8FD8_u539F_u73AF_u5883"><a href="#u8FD8_u539F_u73AF_u5883" class="headerlink" title="还原环境"></a>还原环境</h2><p>最后把我们的环境恢复回去：卸载-&gt;解除映射-&gt;删除image-&gt;删除pool：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo umount /mnt/rbd/qqq</span><br><span class="line">sudo rbd unmap /dev/rbd1</span><br><span class="line">rbd rm ggg/qqq</span><br><span class="line">ceph osd pool delete ggg</span><br></pre></td></tr></table></figure></p>
<p>如果严格按照上面的命令，你应该会在最后一步得到一个错误提示：Error EPERM: WARNING: this will <em>PERMANENTLY DESTROY</em> all data stored in pool ggg.  If you are <em>ABSOLUTELY CERTAIN</em> that is what you want, pass the pool name <em>twice</em>, followed by –yes-i-really-really-mean-it.</p>
<p>删掉pool，里面的数据就真没有啦，所以要谨慎，除了pool名写两遍（重要的事情不应该是三遍么），还得加上<code>--yes-i-really-really-mean-it</code>的免责声明：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool delete ggg ggg --yes-i-really-really-mean-it</span><br></pre></td></tr></table></figure></p>
<p>最后删掉ceph容器（如果你愿意，还有ceph/demo镜像），就当一切都没有发生过：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> ceph</span><br><span class="line">docker rmi ceph/demo</span><br></pre></td></tr></table></figure></p>
<p>悄悄的我走了，正如我悄悄的来；我挥一挥衣袖，不带走一个byte。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://ceph.com/">Ceph</a>是一个高性能的PB级分布式文件系统。它能够在一个系统中提供对象存储、块存储和文件存储。本文的主要内容是怎样用docker从零开始搭建ceph环境，并分别以cephfs和rbd的方式加载它。<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://qinghua.github.io/tags/ceph/"/>
    
      <category term="storage" scheme="http://qinghua.github.io/tags/storage/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如果有10000台机器，你想怎么玩？（二）高可用]]></title>
    <link href="http://qinghua.github.io/kubernetes-in-mesos-2/"/>
    <id>http://qinghua.github.io/kubernetes-in-mesos-2/</id>
    <published>2016-01-01T09:07:07.000Z</published>
    <updated>2016-02-03T13:27:15.000Z</updated>
    <content type="html"><![CDATA[<p>这次聊聊k8s的<a href="http://kubernetes.io/v1.1/docs/admin/high-availability.html" target="_blank" rel="external">高可用性</a>是怎么做的。所谓高可用性，就是在一些服务或机器挂掉了之后集群仍然能正常工作的能力。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a><a id="more"></a>
</li>
</ul>
<p>作为背景知识，先介绍一下<a href="http://kubernetes.io/v1.1/docs/design/architecture.html" target="_blank" rel="external">k8s的架构</a>。<br><img src="http://kubernetes.io/v1.1/docs/design/architecture.png?raw=true" alt=""><br>它分为服务器端（master）和客户端（node）。服务器端主要是3个组件：API Server、Controller Manager和Scheduler。API Server是操作人员和k8s的接口。比如我想看一下当前k8s有几个pod在跑，就需要连接到这个API Server上。Controller Manager顾名思义就是管理各种各样的controller比如先前提到的Replication Controller。Scheduler做的事就是把用户想要启动/删除的pod分发到对应的客户端上。客户端主要是2个组件：Kubelet和Proxy。Kubelet负责响应服务器端的Scheduler分出来的任务。Proxy用来接通服务和对应的机器。举个栗子：如果我们运行这个命令：kubectl -s 192.168.33.10:8080 run nginx —image=nginx来启动一个nginx的rc和pod，API Server（192.168.33.10:8080）就会得到消息并把这些数据存放到<a href="https://github.com/coreos/etcd" target="_blank" rel="external">etcd</a>里。Controller Manager就会去创建rc，Scheduler则会找个客户端，把启动pod的描述放到客户端上的某个文件夹里。客户端上的Kubelet会监视这个文件夹，一旦发现有了新的pod描述文件，便会将这个pod启动起来。多说一句，<a href="http://kubernetes.io/v1.1/docs/admin/kubelet.html" target="_blank" rel="external">Kubelet</a>除了监听文件夹或是某个Url，还有种方式是干脆直接启动一个Http Server让别人来调用。</p>
<p>高可用的情况下，由于用户的命令直接操作的是API Server，所以当API Server挂掉的时候，需要能自动重启。我们可以使用k8s客户端上现成的Kubelet来满足这个需求。Kubelet有一个Standalone模式，把启动API Server的描述文件丢到Kubelet的监视文件夹里就好了。当Kubelet发现API Server挂掉了，就会自动再启动一个API Server，反正新旧API Server连接的存储etcd还是原来那一个。API Server高可用了，要是Kubelet挂了呢？这个…还得监视一下Kubelet…可以用monit之类的东东，这边就不细说了。当然etcd也需要高可用，但是作为分布式存储来说，它的高可用相对而言较为简单并且跟k8s关联不大，这里也不提了。</p>
<p>刚刚提到的都是进程或容器挂掉的高可用。但是万一整个机器都完蛋了，咋办呢？最直接的做法就是整它好几个服务端，一个挂了还有其他的嘛。好几个服务端就有好几个API Server，其中一个为主，其他为从，简单地挂在一个负载均衡如HAProxy上就可以了。如果还嫌HAProxy上可能有单点故障，那就再做负载均衡集群好了，本文不再赘述。API Server可以跑多份，但是Controller Manager和Scheduler现在不建议跑多份。怎么做到呢？官方提供了一个叫做podmaster的镜像，用它启动的容器可以连接到etcd上。当它从etcd上发现当前机器的API Server为主机的时候，便会把Controller Manager和Scheduler的描述文件丢到Kubelet的监视文件夹里，于是Kubelet就会把这俩启动起来。若当前机器的API Server为从机时，它会把Controller Manager和Scheduler的描述文件从Kubelet的监视文件夹里删掉，这样就可以保证整个集群中Controller Manager和Scheduler各只有一份。上面说的这些画到图里就是这样滴：<br><img src="http://kubernetes.io/v1.1/docs/admin/high-availability/ha.png" alt=""></p>
<p>和mesos配合的话，k8s还有<a href="https://github.com/kubernetes/kubernetes/blob/master/contrib/mesos/docs/ha.md" target="_blank" rel="external">另一种高可用方式</a>。这种方式会给Scheduler増加一个叫做–ha的参数，于是Scheduler就能多个同时工作。但是官方也说了，不建议同时起2个以上的Scheduler。这种高可用方式的其它配置还是跟上文所说的一样，照样得使用podmaster，只不过它这回只用管Controller Manager一个而已。</p>
<p>做了这么多，终于把k8s master搞定了。但是还没完，node们还在等着我们呢！如果没用mesos，那就需要把node们的Kubelet重启一下，让它们连接到API Server的负载均衡上去。要是用了mesos就会简单一点儿，因为node们的Kubelet就是由Scheduler帮忙起起来的。记得吗？服务器端我们已经搞定了~</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这次聊聊k8s的<a href="http://kubernetes.io/v1.1/docs/admin/high-availability.html">高可用性</a>是怎么做的。所谓高可用性，就是在一些服务或机器挂掉了之后集群仍然能正常工作的能力。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a>]]>
    
    </summary>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="mesos" scheme="http://qinghua.github.io/tags/mesos/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
</feed>
